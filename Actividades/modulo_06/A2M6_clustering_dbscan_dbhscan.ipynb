{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f267722",
   "metadata": {},
   "source": [
    "# Actividad 2:\n",
    "# Agrupando más allá de las formas: clustering con DBSCAN y DBHSCAN\n",
    "\n",
    "## Objetivo\n",
    "Desarrollar la capacidad de aplicar, comparar y evaluar modelos de clustering basados en densidad (DBSCAN y DBHSCAN) sobre un conjunto de datos con clústeres complejos y ruido, validando los resultados con métricas objetivas y visualización.\n",
    "\n",
    "**Datasets utilizados:**  \n",
    "`Wine`\n",
    "\n",
    "---\n",
    "\n",
    "### Estructura del Notebook:\n",
    "1. Metodología.\n",
    "2. Configuración del entorno.\n",
    "3. Definicion de funciones.\n",
    "4. Uso de funciones y resultados.\n",
    "5. Análisis de los resultados y reflexiones finales.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd718d4b",
   "metadata": {},
   "source": [
    "## 1. Metodología\n",
    "\n",
    "### Flujo de trabajo\n",
    "\n",
    "1. **Carga y preprocesamiento de datos:**\n",
    "    - Se carga el dataset **Wine** y se escala.\n",
    "\n",
    "2. **Busqueda de mejor eps y clustering con DBSCAN y HDBSCAN:**\n",
    "    - Se grafica la curva de distancias para encontrar posibles valores de eps.\n",
    "    - Se prueban 5 valores de eps alrededor del codo de la curva evaluando su silueta y db index para elegir y usar el mejor.\n",
    "    - Se aplica HDBSCAN.\n",
    "\n",
    "\n",
    "3. **Visualización e interpretación:**\n",
    "    - Gráfico de curva de distancias.\n",
    "    - Resumen de la silueta y db index de los 5 valores probados en DBSCAN y del resultado de HDBSCAN.\n",
    "    - Gráficos representando en 2D los resultados de ambos métodos de clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08999f28",
   "metadata": {},
   "source": [
    "# 2. Configuración del entorno\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine, make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e3e3a",
   "metadata": {},
   "source": [
    "# 3. Definición de funciones\n",
    "\n",
    "> **Nota:** Para mejor comprensión de las funciones y su utilidad, esta sección se divide en bloques, en donde cada uno responde a una parte diferente de la metodología de trabajo. \n",
    "\n",
    "---\n",
    "\n",
    "**Bloque 1:** Carga y preprocesamiento de datos.\n",
    "\n",
    "- **`carga_y_preprocesamiento()`** \n",
    "Carga, escala y aplica PCA al dataset.\n",
    "\n",
    "---\n",
    "\n",
    "`Justificación del uso del dataset Wine`\n",
    "\n",
    "Si bien el trabajo daba la opción a elegir entre 3 datasets, Wine fue elegido debido a las siguientes razones:\n",
    "\n",
    "- Datos reales y multidimensionales: A diferencia de los datasets sintéticos como make_moons o make_blobs, Wine contiene 13 variables reales derivadas de análisis químicos de vinos. Esto permite un análisis más realista y relevante en escenarios de clustering.\n",
    "\n",
    "- Aplicación de técnicas de reducción de dimensionalidad: Con Wine es posible aplicar PCA para explorar la estructura interna de datos de alta dimensión, lo cual no sería significativo con make_moons (2D) o make_blobs (datos generados artificialmente sin correlación real).\n",
    "\n",
    "- Evaluación más completa: Al tener datos complejos, es posible evaluar cómo algoritmos como DBSCAN y HDBSCAN se comportan con clusters de densidad y distribución variable, proporcionando un análisis más rico que con datasets sintéticos diseñados para formas simples.\n",
    "\n",
    "---\n",
    "\n",
    "`Justificación del número de componentes PCA`\n",
    "\n",
    "Para seleccionar el número adecuado de componentes principales se evaluó la varianza explicada acumulada con 2, 3 y 4 componentes:\n",
    "\n",
    "- Con 2 componentes, la varianza acumulada es aproximadamente 55%, lo cual es bajo y podría no capturar suficiente información relevante del dataset.\n",
    "\n",
    "- Con 3 componentes, la varianza acumulada aumenta a cerca de 66%, ofreciendo un mejor balance entre reducción dimensional y conservación de información.\n",
    "\n",
    "- Con 4 componentes, la varianza acumulada alcanza cerca del 73%, pero al usar este número se observó que el algoritmo HDBSCAN presenta una silueta muy baja (menor a 0.1), indicando una agrupación menos coherente.\n",
    "\n",
    "Por lo tanto, se decidió usar 3 componentes principales, ya que permiten conservar una proporción significativa de la varianza y a la vez mantienen un rendimiento adecuado en los algoritmos de clustering evaluados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_y_preprocesamiento():\n",
    "    \"\"\"\n",
    "    Carga el dataset Wine desde scikit-learn y aplica escalado estándar a las variables numéricas.\n",
    "\n",
    "    Returns:\n",
    "        X (numpy.ndarray): Matriz de características original sin escalar.\n",
    "        X_scaled (numpy.ndarray): Matriz de características escalada con media 0 y desviación estándar 1.\n",
    "    \"\"\"\n",
    "    data = load_wine()\n",
    "    X = data.data\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7d3db",
   "metadata": {},
   "source": [
    "**Bloque 2:** Busqueda de mejor eps y clustering con DBSCAN y HDBSCAN.\n",
    "\n",
    "- **`plot_k_distance_curve()`** \n",
    "Grafica la curva de distancia con k = 4.\n",
    "\n",
    "- **`dbscan_param_search()`** \n",
    "Evalúa valores de posibles eps extraídos del gráfico de curva de distancias para usar el mejor.\n",
    "\n",
    "- **`hdbscan_cluster()`** \n",
    "Crea el modelo con HDBSCAN.\n",
    "\n",
    "---\n",
    "\n",
    "`Justificación del umbral n_clusters > 1 para Silhouette y Davies-Bouldin en evaluación de eps`\n",
    "\n",
    "- Las métricas Silhouette y Davies-Bouldin Index están diseñadas para evaluar la calidad de la separación entre clústeres.\n",
    "\n",
    "- Silhouette Score compara la distancia promedio de cada punto con los puntos de su propio clúster frente a los puntos del clúster más cercano.\n",
    "Si existe solo un clúster, no hay un \"clúster vecino\" con el cual hacer esta comparación, lo que hace imposible calcular la métrica.\n",
    "\n",
    "- Davies-Bouldin Index se basa en la separación entre cada par de clústeres.\n",
    "Con un único clúster, no existen pares que permitan medir dispersión ni distancia entre grupos.\n",
    "\n",
    "- Por estas razones, cuando n_clusters <= 1 (es decir, solo hay un clúster o todos los puntos fueron clasificados como ruido), estas métricas no se calculan y se devuelven como None, ya que carecen de sentido en dicho contexto.\n",
    "\n",
    "---\n",
    "\n",
    "`Justificación del uso de k = 4 y min_samples = 4`\n",
    "\n",
    "- En el algoritmo DBSCAN, el parámetro min_samples define el número mínimo de puntos necesarios para que una región sea considerada un clúster denso. Para estimar el valor de eps, se utiliza la curva de distancia al k-ésimo vecino, donde el valor de k debe ser coherente con min_samples.\n",
    "\n",
    "- Regla práctica: Los autores de DBSCAN recomiendan usar valores pequeños, típicamente entre 3 y 5, ya que estos permiten detectar agrupaciones pequeñas sin requerir densidades excesivamente altas.\n",
    "\n",
    "- Relación con la dimensionalidad: Se suele tomar min_samples ≈ D + 1, donde D es el número de dimensiones tras la reducción con PCA. En nuestro caso, con 3 componentes principales, el valor min_samples = 4 (y por tanto k = 4) es una elección natural y balanceada.\n",
    "\n",
    "- Consistencia con la curva k-distancia: Probar con valores mayores no aportó ventajas significativas, mientras que k = 4 ofreció una curva clara y un buen punto de codo para seleccionar eps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_distance_curve(X, k=4):\n",
    "    \"\"\"\n",
    "    Grafica la curva de distancia al k-ésimo vecino para estimar eps en DBSCAN.\n",
    "    \n",
    "    Parámetros:\n",
    "        X: array-like, datos preprocesados (p. ej. tras PCA)\n",
    "        k: entero, número de vecinos (min_samples típico)\n",
    "    \"\"\"\n",
    "    nn = NearestNeighbors(n_neighbors=k)\n",
    "    nn.fit(X)\n",
    "    distances, _ = nn.kneighbors(X) # \"_\" representa los índices de los vecino, no se necesitan para la curva\n",
    "    k_distances = np.sort(distances[:, k-1])\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(k_distances)\n",
    "    plt.xlabel(\"Puntos ordenados\")\n",
    "    plt.ylabel(f\"Distancia al {k}º vecino\")\n",
    "    plt.title(\"Curva de distancia para estimar eps\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def dbscan_param_search(X, eps_values, min_samples=4):\n",
    "    \"\"\"\n",
    "    Aplica DBSCAN con distintos eps, evalúa índices y devuelve resultados.\n",
    "    \n",
    "    Parámetros:\n",
    "        X: array-like, datos preprocesados\n",
    "        eps_values: lista o array de floats, valores de eps a probar\n",
    "        min_samples: int, parámetro min_samples de DBSCAN\n",
    "    \n",
    "    Retorna:\n",
    "        resultados: lista de dicts con eps, n_clusters, silhouette, db_index, labels\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "        labels = db.labels_\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        if n_clusters > 1:\n",
    "            sil_score = silhouette_score(X, labels)\n",
    "            db_score = davies_bouldin_score(X, labels)\n",
    "        else:\n",
    "            sil_score = None\n",
    "            db_score = None\n",
    "        \n",
    "        resultados.append({\n",
    "            'eps': eps,\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette': sil_score,\n",
    "            'db_index': db_score,\n",
    "            'labels': labels\n",
    "        })\n",
    "        \n",
    "    return resultados\n",
    "\n",
    "def hdbscan_cluster(X, min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    Aplica HDBSCAN y devuelve etiquetas y métricas.\n",
    "    \n",
    "    Parámetros:\n",
    "        X: array-like, datos preprocesados\n",
    "        min_cluster_size: int, tamaño mínimo del cluster para HDBSCAN\n",
    "    \n",
    "    Retorna:\n",
    "        labels: array de etiquetas\n",
    "        silhouette: float o None\n",
    "        db_index: float o None\n",
    "    \"\"\"\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        sil_score = silhouette_score(X, labels)\n",
    "        db_score = davies_bouldin_score(X, labels)\n",
    "    else:\n",
    "        sil_score = None\n",
    "        db_score = None\n",
    "    \n",
    "    return labels, sil_score, db_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598cb25",
   "metadata": {},
   "source": [
    "**Bloque 3:** Generacion y visualización de resultados con PCA y TSNE.\n",
    "\n",
    "- **`plot_clusters_pca()`** \n",
    "Genera una figura con 2 gráficos correspondientes a la visualización 2d usando PCA.\n",
    "\n",
    "- **`plot_clusters_tnse()`** \n",
    "Genera una figura con 2 gráficos correspondientes a la visualización 2d usando TSNE.\n",
    "\n",
    "---\n",
    "\n",
    "`Justificación para usar solo 2 componentes en la visualización (PCA y t-SNE)`\n",
    "\n",
    "En el análisis principal se utilizan 4 componentes principales (PCA) para capturar una mayor cantidad de varianza y representar mejor la estructura interna de los datos. Esto ayuda a conservar más información relevante para el clustering.\n",
    "\n",
    "Sin embargo, la representación gráfica debe limitarse a dos dimensiones para facilitar la interpretación visual. Por eso, se utilizan dos técnicas diferentes para reducir la dimensionalidad a 2D para la visualización:\n",
    "\n",
    "- PCA 2D:\n",
    "Seleccionamos las primeras dos componentes principales directamente del resultado PCA de 4 componentes. Estas componentes explican la mayor parte de la varianza acumulada y permiten observar la estructura general y la separación de los clusters en un espacio lineal. Es una forma rápida, lineal y consistente de representar los datos para visualización.\n",
    "\n",
    "- t-SNE 2D:\n",
    "Como técnica no lineal y estocástica, t-SNE transforma los datos ya reducidos (en este caso, las 4 componentes PCA) a un espacio bidimensional buscando preservar relaciones de proximidad local. Esto puede mostrar agrupamientos y estructuras complejas que PCA lineal no detecta, aunque puede distorsionar la forma global y las distancias absolutas.\n",
    "Esta transformación adicional ayuda a visualizar mejor clusters con formas irregulares o con separaciones no lineales.\n",
    "\n",
    "Por lo tanto, combinar ambas visualizaciones ofrece una comprensión más completa: la visualización PCA para la estructura general, y la visualización t-SNE para detalles finos y separación local de clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_pca(X_pca, labels_db, labels_hdb, eps):\n",
    "    \"\"\"\n",
    "    Muestra en una sola figura dos gráficos comparando DBSCAN y HDBSCAN.\n",
    "    \n",
    "    Parámetros:\n",
    "        X_2d: array-like, datos con 2 dimensiones (ej. PCA con 2 comps)\n",
    "        labels_db: array-like, etiquetas de cluster de DBSCAN\n",
    "        labels_hdb: array-like, etiquetas de cluster de HDBSCAN\n",
    "        eps: float, valor de eps usado en DBSCAN\n",
    "    \"\"\"\n",
    "    # Asegurarse de que, independiente del numero de componentes, X_pca muestre solo 2 dimensiones\n",
    "    X_pca_2d = X_pca[:, :2]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- DBSCAN ---\n",
    "    unique_labels_db = np.unique(labels_db)\n",
    "    palette_db = sns.color_palette('tab10', len(unique_labels_db))\n",
    "    colors_db = [palette_db[i] if i != -1 else (0, 0, 0) for i in labels_db]\n",
    "\n",
    "    axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=colors_db, s=50, alpha=0.7)\n",
    "    axes[0].set_title(f\"DBSCAN (eps={eps})\")\n",
    "    axes[0].set_xlabel(\"Componente 1\")\n",
    "    axes[0].set_ylabel(\"Componente 2\")\n",
    "\n",
    "    # --- HDBSCAN ---\n",
    "    unique_labels_hdb = np.unique(labels_hdb)\n",
    "    palette_hdb = sns.color_palette('tab10', len(unique_labels_hdb))\n",
    "    colors_hdb = [palette_hdb[i] if i != -1 else (0, 0, 0) for i in labels_hdb]\n",
    "\n",
    "    axes[1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=colors_hdb, s=50, alpha=0.7)\n",
    "    axes[1].set_title(\"HDBSCAN\")\n",
    "    axes[1].set_xlabel(\"Componente 1\")\n",
    "    axes[1].set_ylabel(\"Componente 2\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_clusters_tsne(X_pca, labels_db, labels_hdb, eps):\n",
    "    \"\"\"\n",
    "    Muestra en una sola figura dos gráficos comparando DBSCAN y HDBSCAN.\n",
    "    \n",
    "    Parámetros:\n",
    "        X_pca: array-like, datos reducidos con PCA (ej. 4 componentes)\n",
    "        labels_db: array-like, etiquetas de cluster de DBSCAN\n",
    "        labels_hdb: array-like, etiquetas de cluster de HDBSCAN\n",
    "        eps: float, valor de eps usado en DBSCAN\n",
    "    \"\"\"\n",
    "    # Reducir a 2D con t-SNE para mejor visualización\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne_2d = tsne.fit_transform(X_pca)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- DBSCAN ---\n",
    "    unique_labels_db = np.unique(labels_db)\n",
    "    palette_db = sns.color_palette('tab10', len(unique_labels_db))\n",
    "    colors_db = [palette_db[i] if i != -1 else (0, 0, 0) for i in labels_db]\n",
    "\n",
    "    axes[0].scatter(X_tsne_2d[:, 0], X_tsne_2d[:, 1], c=colors_db, s=50, alpha=0.7)\n",
    "    axes[0].set_title(f\"DBSCAN (eps={eps})\")\n",
    "    axes[0].set_xlabel(\"Dimensión 1 (t-SNE)\")\n",
    "    axes[0].set_ylabel(\"Dimensión 2 (t-SNE)\")\n",
    "\n",
    "    # --- HDBSCAN ---\n",
    "    unique_labels_hdb = np.unique(labels_hdb)\n",
    "    palette_hdb = sns.color_palette('tab10', len(unique_labels_hdb))\n",
    "    colors_hdb = [palette_hdb[i] if i != -1 else (0, 0, 0) for i in labels_hdb]\n",
    "\n",
    "    axes[1].scatter(X_tsne_2d[:, 0], X_tsne_2d[:, 1], c=colors_hdb, s=50, alpha=0.7)\n",
    "    axes[1].set_title(\"HDBSCAN\")\n",
    "    axes[1].set_xlabel(\"Dimensión 1 (t-SNE)\")\n",
    "    axes[1].set_ylabel(\"Dimensión 2 (t-SNE)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057e236",
   "metadata": {},
   "source": [
    "**Bloque 4:** Función de ejecución del código.\n",
    "\n",
    "- **`main()`** \n",
    "Ejecuta el flujo completo de análisis y visualización de clustering con DBSCAN y HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f11bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que orquesta la carga, preprocesamiento, análisis PCA, \n",
    "    optimización de hiperparámetros, entrenamiento, evaluación y visualización \n",
    "    de modelos KNN con y sin reducción dimensional mediante PCA sobre el dataset Wine.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y preprocesar (escalar, PCA 4 comps)\n",
    "    X_pca = carga_y_preprocesamiento()\n",
    "\n",
    "    # 2. Graficar curva k-distancia para estimar eps\n",
    "    print(\"Curva de distancia al k-ésimo vecino para estimar eps\")\n",
    "    plot_k_distance_curve(X_pca, k=4)\n",
    "\n",
    "    # 3. Buscar mejor eps para DBSCAN\n",
    "    eps_range = [0.7, 0.75, 0.8, 0.85, 0.9]  # o el rango que decidas\n",
    "    resultados_dbscan = dbscan_param_search(X_pca, eps_range, min_samples=4)\n",
    "\n",
    "    # Mostrar resumen sin etiquetas\n",
    "    print(\"\\nResultados de evaluación de valores eps en DBSCAN:\")\n",
    "    for r in resultados_dbscan:\n",
    "        resumen = {k: v for k, v in r.items() if k != 'labels'}  # no mostrar labels\n",
    "        print(resumen)\n",
    "\n",
    "    # 4. Elegir el mejor eps (por ejemplo 0.85) y obtener sus labels\n",
    "    best_eps = 0.85\n",
    "    best_labels = [r['labels'] for r in resultados_dbscan if r['eps'] == best_eps][0]\n",
    "\n",
    "    # 5. Aplicar HDBSCAN\n",
    "    labels_hdbscan, sil_hdbscan, db_hdbscan = hdbscan_cluster(X_pca)\n",
    "\n",
    "    print(\"\\nResultados de HDBSCAN:\")\n",
    "    print(f\"Clusters: {len(set(labels_hdbscan)) - (1 if -1 in labels_hdbscan else 0)}, \"\n",
    "          f\"Silhouette: {sil_hdbscan}, DB Index: {db_hdbscan}\")\n",
    "\n",
    "    # 6. Visualizar resultados con los 2 primeros componentes PCA\n",
    "    print(\"\\nVicualización de resultados con PCA y t-SNE\")\n",
    "    plot_clusters_pca(X_pca, best_labels, labels_hdbscan, best_eps)\n",
    "    plot_clusters_tsne(X_pca, best_labels, labels_hdbscan, best_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70a949",
   "metadata": {},
   "source": [
    "# 4. Visualización de resultados\n",
    "\n",
    "Se muestran los resultados obtenidos a partir de la ejecución de la funcion **main()**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c456a",
   "metadata": {},
   "source": [
    "# 5. Análisis de los resultados y reflexiones finales\n",
    "\n",
    "---\n",
    "\n",
    "### Elección del parámetro eps en DBSCAN y análisis de silueta y db index\n",
    "\n",
    "La elección del parámetro eps = 0.85 se fundamenta en un balance entre la calidad interna de los clusters y la cantidad razonable de grupos generados:\n",
    "\n",
    "- Aunque valores mayores de eps, como 0.9, presentan un índice de silueta más alto (0.28), la cantidad de clusters cae a solo 2, lo que limita el detalle del agrupamiento.\n",
    "\n",
    "- Por otro lado, valores menores como 0.7 generan más clusters (9) pero con una silueta mucho más baja (0.07), lo que indica agrupaciones menos definidas.\n",
    "\n",
    "- Con eps = 0.85 se obtuvo un índice de silueta intermedio-alto (0.24) con 4 clusters, lo cual representa un compromiso adecuado para capturar estructura en los datos sin perder cohesión interna ni generar un número excesivo de grupos poco significativos.\n",
    "\n",
    "Por otro lado, HDBSCAN produjo 3 clusters con un índice de silueta ligeramente menor (0.20), lo que refleja agrupamientos algo menos cohesionados. Sin embargo, HDBSCAN es un algoritmo que maneja mejor ruido y clusters de forma irregular, y su capacidad para identificar puntos considerados ruido (-1 en etiquetas) puede hacer que el número efectivo de clusters sea menor, pero de mayor calidad en términos de diferenciación natural en los datos.\n",
    "\n",
    "En términos del índice Davies-Bouldin (DB index), un valor menor indica clusters más separados y compactos. Aquí, HDBSCAN mostró un DB index moderadamente mejor (1.81) que DBSCAN con eps=0.85 (2.11), sugiriendo que aunque la silueta sea menor, la separación entre clusters es mejor con HDBSCAN. Esto puede indicar que HDBSCAN está formando clusters con límites más claros, aunque internamente sean menos densos o compactos que los clusters de DBSCAN.\n",
    "\n",
    "En resumen, DBSCAN con eps=0.85 ofrece un equilibrio entre cantidad de clusters y cohesión interna, mientras que HDBSCAN proporciona una agrupación que puede ser más robusta a ruido y estructuras irregulares, con mejor separación pero menor cohesión interna.\n",
    "\n",
    "---\n",
    "\n",
    "### Consideraciones sobre la visualización en 2D (PCA vs t-SNE)\n",
    "\n",
    "Para visualizar los clusters se redujo la dimensionalidad a dos dimensiones utilizando dos métodos distintos:\n",
    "\n",
    "- **PCA (2 componentes principales):** conserva la mayor varianza lineal posible y muestra los clusters distribuidos con formas más alineadas a los ejes, formando una estructura tipo \"V\", con grupos ubicados en diferentes cuadrantes.\n",
    "\n",
    "- **t-SNE aplicado sobre PCA:** genera una representación no lineal que agrupa mejor los puntos similares, mostrando clusters más compactos y separados espacialmente en el plano 2D, con una distribución que refleja mejor las relaciones locales en los datos, aunque con escalas y rangos más amplios y menos interpretables en términos absolutos.\n",
    "\n",
    "Esta diferencia explica por qué los clusters pueden verse distribuidos de manera distinta en cada visualización, siendo t-SNE útil para resaltar agrupamientos naturales no lineales, mientras que PCA es más útil para una interpretación general y lineal de la estructura.\n",
    "\n",
    "---\n",
    "\n",
    "## Reflexiones finales\n",
    "\n",
    "La elección del método depende del objetivo del análisis: si se prioriza la robustez frente a ruido y clusters con formas irregulares, HDBSCAN puede ser la mejor opción; mientras que para obtener agrupamientos más definidos y un número controlado de clusters, DBSCAN con eps=0.85 resulta más adecuado.\n",
    "\n",
    "### ¿Qué algoritmo funcionó mejor?\n",
    "\n",
    "El algoritmo DBSCAN con eps = 0.85 logró el mejor balance en cuanto a cohesión (silhouette 0.24) y cantidad adecuada de clusters (4), superando en silueta a HDBSCAN (0.20) y manteniendo un número razonable de grupos. Aunque HDBSCAN muestra un DB index menor que algunos valores de DBSCAN, su silueta es inferior a la mejor configuración DBSCAN.\n",
    "\n",
    "### Limitaciones encontradas:\n",
    "\n",
    "La principal limitación fue la sensibilidad a la selección del parámetro eps en DBSCAN, que requiere una búsqueda cuidadosa para evitar obtener solo un cluster o demasiados clusters poco coherentes. Además, la elección del número de componentes PCA afecta significativamente la calidad de los resultados, y no siempre aumentar la dimensionalidad mejora el agrupamiento.\n",
    "\n",
    "### Posibles mejoras:\n",
    "\n",
    "Para futuras iteraciones se recomienda explorar técnicas de selección automática de eps más robustas, o combinar DBSCAN con métodos de reducción dimensional que preserven mejor la estructura, como t-SNE o UMAP. También se podría investigar un análisis más profundo de la estabilidad de clusters y validación externa si hubiera etiquetas disponibles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
