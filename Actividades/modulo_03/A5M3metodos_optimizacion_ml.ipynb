{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb11cf5d",
   "metadata": {},
   "source": [
    "# Actividad 5:\n",
    "# Comparación Práctica de Métodos de Optimización en Regresión Lineal\n",
    "\n",
    "---\n",
    "\n",
    "> **Importante:** Este notebook debe ejecutarse en el orden sugerido para evitar errores. Algunas funciones dependen de bloques anteriores y podrían no estar disponibles si se omite su ejecución previa.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura:\n",
    "1. Decisiones de diseño.\n",
    "2. Funciones definidas.\n",
    "3. Uso de las funciones y resultados.\n",
    "4. Análisis de resultados y reflexiones.\n",
    "\n",
    "---\n",
    "\n",
    "## Decisiones de diseño\n",
    "\n",
    "---\n",
    "\n",
    "Consistió en implementar y comparar tres algoritmos de optimización para resolver un problema de regresión lineal simple utilizando datos sintéticos generados artificialmente.\n",
    "\n",
    "Se realizaron los siguientes pasos:\n",
    "\n",
    "- Generación de datos sintéticos:\n",
    "Se simularon 100 observaciones con una relación lineal, lo que permite tener un escenario controlado y reproducible.\n",
    "\n",
    "- Definición de la función de costo:\n",
    "Se utilizó el error cuadrático medio (MSE) como métrica de evaluación del desempeño del modelo.\n",
    "\n",
    "- Cálculo del gradiente:\n",
    "Se derivó la función de costo respecto a los parámetros, obteniendo así las expresiones necesarias para los algoritmos de optimización.\n",
    "\n",
    "- Aplicación de tres métodos de optimización:\n",
    "    - **GD (Gradiente Descendente):** Usa todo el conjunto de datos por iteración.\n",
    "    - **SGD (Estocástico):** Usa un solo punto aleatorio en cada paso.\n",
    "    - **Adam:** Combina momentum y tasas de aprendizaje adaptativas.\n",
    "\n",
    "- Visualización y comparación de resultados:\n",
    "Se graficó la evolución del error (MSE) y la trayectoria de los parámetros para cada optimizador, además de mostrar los resultados en un DataFrame de pandas para comparar los tres métodos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26355bbe",
   "metadata": {},
   "source": [
    "# Funciones definidas\n",
    "\n",
    "---\n",
    "\n",
    "Este proyecto se estructura en siete funciones principales, las cuales fueron divididas en tres bloques:\n",
    "\n",
    "**Bloque 1: Generación de Datos y Funciones Base**\n",
    "\n",
    "- **`generar_datos_sinteticos()`**  \n",
    "  Genera un conjunto reproducible de datos sintéticos para regresión lineal, con ruido gaussiano añadido.\n",
    "\n",
    "- **`error_cuadratico_medio()`**  \n",
    "  Calcula el error cuadrático medio (MSE) entre las predicciones del modelo lineal y los valores reales.\n",
    "\n",
    "- **`gradientes_mse()`**  \n",
    "  Calcula los gradientes de la función de costo MSE con respecto a los parámetros pendiente y sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd541aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Generación de Datos Sintéticos\n",
    "def generar_datos_sinteticos(n_muestras=100, semilla=42):\n",
    "    \"\"\"\n",
    "    Genera datos sintéticos para un problema de regresión lineal simple con ruido gaussiano.\n",
    "\n",
    "    Args:\n",
    "        n_muestras (int): Número de muestras a generar.\n",
    "        semilla (int): Semilla para reproducibilidad de los datos.\n",
    "\n",
    "    Returns:\n",
    "        x (np.ndarray): Variable independiente, valores entre 0 y 10.\n",
    "        y (np.ndarray): Variable dependiente con ruido.\n",
    "        w_real (float): Valor real de la pendiente del modelo.\n",
    "        b_real (float): Valor real del sesgo del modelo.\n",
    "    \"\"\"\n",
    "    np.random.seed(semilla)\n",
    "    x = np.random.uniform(0, 10, n_muestras) # # Generación de datos aleatorios uniformemente distribuidos entre 0 y 10\n",
    "    w_real, b_real = 2.5, 1.0 # w = peso/weight, b = sesgo/bias\n",
    "    ruido = np.random.normal(0, 1, n_muestras)\n",
    "    y = w_real * x + b_real + ruido\n",
    "    return x, y, w_real, b_real\n",
    "\n",
    "\n",
    "# Función de Costo y Gradientes\n",
    "def funcion_costo(w, b, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el Error Cuadrático Medio (MSE) entre las predicciones y los valores reales.\n",
    "\n",
    "    Args:\n",
    "        w (float): Pendiente actual del modelo.\n",
    "        b (float): Sesgo actual del modelo.\n",
    "        x (np.ndarray): Variable independiente.\n",
    "        y (np.ndarray): Variable dependiente real.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor del MSE para los parámetros dados.\n",
    "    \"\"\"\n",
    "    y_pred = w * x + b\n",
    "    return np.mean((y - y_pred) ** 2) \n",
    "\n",
    "def gradientes_mse(w, b, x, y):\n",
    "    \"\"\"\n",
    "    Calcula los gradientes del Error Cuadrático Medio respecto a la pendiente y el sesgo.\n",
    "\n",
    "    Args:\n",
    "        w (float): Pendiente actual del modelo.\n",
    "        b (float): Sesgo actual del modelo.\n",
    "        x (np.ndarray): Variable independiente.\n",
    "        y (np.ndarray): Variable dependiente real.\n",
    "\n",
    "    Returns:\n",
    "        grad_w (float): Gradiente del MSE respecto a la pendiente (∂MSE/∂w).\n",
    "        grad_b (float): Gradiente del MSE respecto al sesgo (∂MSE/∂b).\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    error = y - (w * x + b)\n",
    "    grad_w = (-2 / n) * np.sum(x * error)\n",
    "    grad_b = (-2 / n) * np.sum(error)\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc15620",
   "metadata": {},
   "source": [
    "**Bloque 2: Métodos de Optimización**\n",
    "\n",
    "- **`descenso_gradiente()`, `descenso_gradiente_estocastico()` y `optimizador_adam()`**  \n",
    "  Implementan tres métodos de optimización para minimizar la función de costo: descenso de gradiente batch, descenso de gradiente estocástico y Adam, respectivamente.\n",
    "\n",
    "> Cabe destacar que el optimizador **Adam fue implementado manualmente** (sin usar librerías como PyTorch) para mantener la coherencia estructural del código base y facilitar su integración con las demás funciones. Esta decisión se tomó especialmente porque Adam fue el último método en añadirse al proyecto debido a que era opcional, y rehacer el flujo completo con un framework externo podría haber requerido reescrituras innecesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dde862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_gradiente(x, y, tasa_aprendizaje=0.01, iteraciones=1000):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo de Descenso de Gradiente (GD) para optimizar parámetros de regresión lineal.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Variable independiente.\n",
    "        y (np.ndarray): Variable dependiente real.\n",
    "        tasa_aprendizaje (float): Tasa de aprendizaje (learning rate).\n",
    "        iteraciones (int): Número de iteraciones para el algoritmo.\n",
    "\n",
    "    Returns:\n",
    "        w (float): Valor final de la pendiente (weight) del modelo.\n",
    "        b (float): Valor final del sesgo (bias) del modelo.\n",
    "        historial (dict): Diccionario que almacena el historial de los valores de w, b y el error (MSE)\n",
    "                          en cada iteración. Contiene las claves 'w', 'b' y 'error'.\n",
    "    \"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    historial = {'w': [], 'b': [], 'error': []}\n",
    "    for _ in range(iteraciones):\n",
    "        grad_w, grad_b = gradientes_mse(w, b, x, y)\n",
    "        w -= tasa_aprendizaje * grad_w\n",
    "        b -= tasa_aprendizaje * grad_b\n",
    "        error = funcion_costo(w, b, x, y)\n",
    "        historial['w'].append(w)\n",
    "        historial['b'].append(b)\n",
    "        historial['error'].append(error)\n",
    "    return w, b, historial\n",
    "\n",
    "\n",
    "def descenso_gradiente_estocastico(x, y, tasa_aprendizaje=0.01, iteraciones=1000):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo de Descenso de Gradiente Estocástico (SGD) para optimizar parámetros.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Variable independiente.\n",
    "        y (np.ndarray): Variable dependiente real.\n",
    "        tasa_aprendizaje (float): Tasa de aprendizaje.\n",
    "        iteraciones (int): Número de iteraciones.\n",
    "\n",
    "    Returns:\n",
    "        w (float): Valor final de la pendiente (weight) del modelo.\n",
    "        b (float): Valor final del sesgo (bias) del modelo.\n",
    "        historial (dict): Diccionario con el historial de los valores de w, b y error (MSE)\n",
    "                          por iteración. Contiene las claves 'w', 'b' y 'error'.\n",
    "    \"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    historial = {'w': [], 'b': [], 'error': []}\n",
    "    n = len(x)\n",
    "    for _ in range(iteraciones):\n",
    "        i = np.random.randint(0, n)\n",
    "        xi, yi = x[i], y[i]\n",
    "        pred = w * xi + b\n",
    "        grad_w = -2 * xi * (yi - pred)\n",
    "        grad_b = -2 * (yi - pred)\n",
    "        w -= tasa_aprendizaje * grad_w\n",
    "        b -= tasa_aprendizaje * grad_b\n",
    "        error = funcion_costo(w, b, x, y)\n",
    "        historial['w'].append(w)\n",
    "        historial['b'].append(b)\n",
    "        historial['error'].append(error)\n",
    "    return w, b, historial\n",
    "\n",
    "\n",
    "def optimizador_adam(x, y, tasa_aprendizaje=0.01, iteraciones=1000, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Implementa el optimizador Adam para la regresión lineal.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Variable independiente.\n",
    "        y (np.ndarray): Variable dependiente real.\n",
    "        tasa_aprendizaje (float): Tasa de aprendizaje.\n",
    "        iteraciones (int): Número de iteraciones.\n",
    "        beta1 (float): Parámetro de decaimiento para el primer momento.\n",
    "        beta2 (float): Parámetro de decaimiento para el segundo momento.\n",
    "        epsilon (float): Pequeño valor para evitar división por cero.\n",
    "\n",
    "    Returns:\n",
    "        w (float): Valor final de la pendiente (weight) del modelo.\n",
    "        b (float): Valor final del sesgo (bias) del modelo.\n",
    "        historial (dict): Diccionario que contiene el historial de los valores de w, b y el error (MSE)\n",
    "                          en cada iteración. Claves: 'w', 'b' y 'error'.\n",
    "    \"\"\"\n",
    "    w, b = 0.0, 0.0\n",
    "    m_w, v_w, m_b, v_b = 0.0, 0.0, 0.0, 0.0\n",
    "    historial = {'w': [], 'b': [], 'error': []}\n",
    "    \n",
    "    for t in range(1, iteraciones + 1):\n",
    "        grad_w, grad_b = gradientes_mse(w, b, x, y)\n",
    "        \n",
    "        # Actualizar momentos de primer orden (momentum)\n",
    "        m_w = beta1 * m_w + (1 - beta1) * grad_w\n",
    "        m_b = beta1 * m_b + (1 - beta1) * grad_b\n",
    "        \n",
    "        # Actualizar momentos de segundo orden (RMSprop)\n",
    "        v_w = beta2 * v_w + (1 - beta2) * (grad_w ** 2)\n",
    "        v_b = beta2 * v_b + (1 - beta2) * (grad_b ** 2)\n",
    "        \n",
    "        # Corrección de sesgo para los momentos\n",
    "        m_w_corr = m_w / (1 - beta1 ** t)\n",
    "        m_b_corr = m_b / (1 - beta1 ** t)\n",
    "        v_w_corr = v_w / (1 - beta2 ** t)\n",
    "        v_b_corr = v_b / (1 - beta2 ** t)\n",
    "        \n",
    "        # Actualizar parámetros\n",
    "        w -= tasa_aprendizaje * m_w_corr / (np.sqrt(v_w_corr) + epsilon)\n",
    "        b -= tasa_aprendizaje * m_b_corr / (np.sqrt(v_b_corr) + epsilon)\n",
    "        \n",
    "        error = funcion_costo(w, b, x, y)\n",
    "        historial['w'].append(w)\n",
    "        historial['b'].append(b)\n",
    "        historial['error'].append(error)\n",
    "        \n",
    "    return w, b, historial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbdac8",
   "metadata": {},
   "source": [
    "**Bloque 3: Visualización**\n",
    "\n",
    "- **`visualizar_resultados()`**  \n",
    "  Grafica la evolución del error y la trayectoria de los parámetros a lo largo de las iteraciones para cada método, facilitando la comparación de su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_resultados(hist_gd, hist_sgd, hist_adam):\n",
    "    \"\"\"\n",
    "    Grafica la evolución del error y la trayectoria de los parámetros para los métodos de optimización.\n",
    "\n",
    "    Args:\n",
    "        hist_gd (dict): Historial con parámetros y errores para Descenso de Gradiente.\n",
    "        hist_sgd (dict): Historial para Descenso de Gradiente Estocástico.\n",
    "        hist_adam (dict): Historial para optimizador Adam.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Evolución del error\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist_gd['error'], label='Gradiente')\n",
    "    plt.plot(hist_sgd['error'], label='Estocástico')\n",
    "    plt.plot(hist_adam['error'], label='Adam')\n",
    "    plt.xlabel('Iteración')\n",
    "    plt.ylabel('Error cuadrático medio')\n",
    "    plt.title('Evolución del error')\n",
    "    plt.legend()\n",
    "\n",
    "    # Trayectoria de parámetros\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist_gd['w'], hist_gd['b'], label='Gradiente')\n",
    "    plt.plot(hist_sgd['w'], hist_sgd['b'], label='Estocástico')\n",
    "    plt.plot(hist_adam['w'], hist_adam['b'], label='Adam')\n",
    "    plt.xlabel('Pendiente (w)')\n",
    "    plt.ylabel('Sesgo (b)')\n",
    "    plt.title('Trayectoria de parámetros')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparacion_metodos.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291df13",
   "metadata": {},
   "source": [
    "# Uso de funciones\n",
    "\n",
    "---\n",
    "\n",
    "Se aplican las funciones definidas y se muestran los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ffbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w_real, b_real = generar_datos_sinteticos()\n",
    "iteraciones = 1000\n",
    "# Para efectos prácticos, pueden probarse tasas distintas para cada modelo, aunque pareciera que 0.01 es la mejor en general.\n",
    "tasas = {\n",
    "    'GD': 0.01,\n",
    "    'SGD': 0.01,\n",
    "    'Adam': 0.01\n",
    "}\n",
    "\n",
    "w_gd, b_gd, hist_gd = descenso_gradiente(x, y, tasas['GD'], iteraciones)\n",
    "w_sgd, b_sgd, hist_sgd = descenso_gradiente_estocastico(x, y, tasas['SGD'], iteraciones)\n",
    "w_adam, b_adam, hist_adam = optimizador_adam(x, y, tasas['Adam'], iteraciones)\n",
    "\n",
    "# Crear DataFrame de resultados incluyendo valores reales\n",
    "df_resultados = pd.DataFrame({\n",
    "    'Optimizador': ['Gradiente', 'Estocástico', 'Adam', 'Valor Real'],\n",
    "    'Tasa de Aprendizaje': [tasas['GD'], tasas['SGD'], tasas['Adam'], None],\n",
    "    'Iteraciones': [iteraciones, iteraciones, iteraciones, None],\n",
    "    'Pendiente (w)': [w_gd, w_sgd, w_adam, w_real],\n",
    "    'Intersección (b)': [b_gd, b_sgd, b_adam, b_real],\n",
    "    'Error MSE': [hist_gd['error'][-1], hist_sgd['error'][-1], hist_adam['error'][-1], None]\n",
    "})\n",
    "\n",
    "df_resultados = df_resultados.round(4)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n=== Resultados Finales por Optimizador (y valores reales) ===\\n\")\n",
    "print(df_resultados)\n",
    "\n",
    "# Visualizar comparaciones\n",
    "visualizar_resultados(hist_gd, hist_sgd, hist_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cb3fc",
   "metadata": {},
   "source": [
    "# Análisis de Resultados y Reflexiones\n",
    "\n",
    "---\n",
    "\n",
    "## Resultados Finales\n",
    "\n",
    "A continuación se muestran los resultados de los tres optimizadores aplicados a una regresión lineal con datos sintéticos, junto con los valores reales de la pendiente (`w`) e intersección (`b`):\n",
    "\n",
    "| Optimizador | Tasa de Aprendizaje | Iteraciones | Pendiente (w) | Intersección (b) | Error MSE |\n",
    "|-------------|---------------------|-------------|----------------|------------------|-----------|\n",
    "| Gradiente   | 0.01                | 1000        | 2.45           | 1.21             | 0.81      |\n",
    "| Estocástico | 0.01                | 1000        | 2.43           | 1.32             | 0.81      |\n",
    "| Adam        | 0.01                | 1000        | 2.34           | 1.90             | 0.94      |\n",
    "| **Valor Real** | -               | -           | **2.50**       | **1.00**         | -         |\n",
    "\n",
    "---\n",
    "\n",
    "## Evolución del Error (Gráfico Izquierdo)\n",
    "\n",
    "- **Gradiente Descendente (GD)** y **Estocástico (SGD)** convergen rápidamente a un error cuadrático medio (MSE) bajo, estabilizándose antes de las 100 iteraciones.\n",
    "- **Adam** muestra una convergencia más lenta. aunque su error sigue disminuyendo de forma gradual y no se estabiliza completamente hasta después de las ~300 iteraciones, tarda más en estabilizarse, alcanzando un MSE ligeramente superior a los otros métodos.\n",
    "\n",
    "---\n",
    "\n",
    "## Trayectoria de Parámetros (Gráfico Derecho)\n",
    "\n",
    "- **GD** muestra una trayectoria estable y directa hacia los valores óptimos de los parámetros.\n",
    "- **SGD** tiene una trayectoria mucho más ruidosa, con oscilaciones, lo cual es esperado por su naturaleza basada en ejemplos individuales.\n",
    "- **Adam** presenta un avance más progresivo y estable, explorando el espacio de parámetros de forma controlada, aunque tarda más en llegar a la zona óptima.\n",
    "\n",
    "---\n",
    "\n",
    "## Reflexiones sobre Tasa de Aprendizaje y Métodos\n",
    "\n",
    "- La **tasa de aprendizaje (α)** es un hiperparámetro crítico:\n",
    "  - Muy alta → inestabilidad.\n",
    "  - Muy baja → convergencia lenta o estancada.\n",
    "- **GD** es ideal para conjuntos de datos pequeños y cuando se requiere precisión.\n",
    "- **SGD** es eficiente para grandes volúmenes de datos, aunque ruidoso.\n",
    "- **Adam** adapta la tasa de aprendizaje automáticamente, combinando lo mejor de GD y SGD, y suele ser robusto en tareas más complejas.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "Este ejercicio demuestra cómo un mismo problema de regresión puede dar resultados distintos dependiendo del optimizador y la tasa de aprendizaje usada. Ajustar correctamente estos parámetros es esencial para lograr eficiencia, estabilidad y precisión en el entrenamiento.\n",
    "\n",
    "- Si bien con tasas de 0.01 los tres métodos mostraron buenos resultados, **cabe destacar que al utilizar tasas más pequeñas como 0.005 o 0.001, el optimizador Adam ve un deterioro significativo en su desempeño**, aumentando su MSE a más de 1 y hasta **80** respectivamente. Esto **evidencia la sensibilidad de cada optimizador a su tasa de aprendizaje**, y subraya la importancia crítica de ajustarla adecuadamente. Incluso modelos considerados robustos, como Adam, pueden fallar si los hiperparámetros no son elegidos con cuidado.\n",
    "\n",
    "> En este caso, con una tasa de 0.01 y 1000 iteraciones, **todos los métodos alcanzaron un MSE menor a 1**, siendo **Adam el más lento en converger**, pero también el más suave en su evolución."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
