{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461a7cfc",
   "metadata": {},
   "source": [
    "# Módulo 7 - Actividad 3:\n",
    "# Reconstrucción de imágenes y eliminación de ruido usando Autoencoders\n",
    "\n",
    "## Objetivo\n",
    "Comparar el desempeño de ambos modelos y analizar visualmente los resultados obtenidos.\n",
    "\n",
    "**Datasets utilizados:**  \n",
    "`MNIST`\n",
    "\n",
    "---\n",
    "\n",
    "### Estructura del Notebook:\n",
    "1. Metodología.\n",
    "2. Configuración del entorno.\n",
    "3. Definicion de funciones.\n",
    "4. Uso de funciones y resultados.\n",
    "5. Análisis de los resultados y reflexiones finales.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52725bad",
   "metadata": {},
   "source": [
    "## 1. Metodología\n",
    "\n",
    "### Flujo de trabajo\n",
    "\n",
    "> Para facilitar la comprensión del código, cada etapa del proceso está modularizada en funciones, integrando la carga de datos, creación y entrenamiento de modelos, y visualización de resultados. Esto permite entender el papel específico de cada bloque dentro del flujo general de autoencoders.\n",
    "\n",
    "1. **Carga y preprocesamiento de datos:**\n",
    "    - Se carga el dataset **MNIST**, normalizando las imágenes y aplanándolas para adecuarlas a la entrada del autoencoder.\n",
    "\n",
    "2. **Construcción y entrenamiento del autoencoder básico:**\n",
    "    - Se crea un autoencoder con arquitectura 784 - 128 - 64 - 128 - 784.\n",
    "    - El modelo se entrena con imágenes limpias para aprender la reconstrucción directa.\n",
    "\n",
    "3. **Generación de ruido y entrenamiento del denoising autoencoder:**\n",
    "    - Se generan versiones ruidosas del dataset aplicando ruido gaussiano y manteniendo los valores en el rango [0, 1].\n",
    "    - Se entrena el mismo modelo de autoencoder para que aprenda a reconstruir imágenes limpias a partir de entradas ruidosas.\n",
    "\n",
    "4. **Comparación y visualización de pérdidas y reconstrucciones entre modelos:**\n",
    "    - Se visualiza la comparación de la evolución de la pérdida de reconstrucción durante el entrenamiento del Autoencoder básico y del Denoising Autoencoder.\n",
    "    - Se visualizan las imágenes originales, reconstruidas por Autoencoder básico, ruidosas y las reconstruidas para comprobar la eficacia del denoising autoencoder.\n",
    "    - Se construye una tabla comparativa con las pérdidas y pérdidas de validación de ambos modelos a lo largo de las épocas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe34962",
   "metadata": {},
   "source": [
    "# 2. Configuración del entorno\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import display\n",
    "\n",
    "# Establecer seeds\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ec445",
   "metadata": {},
   "source": [
    "# 3. Definición de funciones\n",
    "\n",
    "> **Nota:** Para mejor comprensión de las funciones y su utilidad, esta sección se divide en bloques, en donde cada uno responde a una parte diferente de la metodología de trabajo. \n",
    "\n",
    "---\n",
    "\n",
    "**Bloque 1:** Carga y preprocesamiento de datos.\n",
    "\n",
    "- **`cargar_datos()`** \n",
    "Carga y preprocesa el dataset MNIST en formato vectorizado.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c27a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos():\n",
    "    \"\"\"\n",
    "    Carga el dataset MNIST, normaliza los valores a [0, 1] y aplana las imágenes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x_train, x_test) con las imágenes procesadas en formato (num_samples, 784).\n",
    "    \"\"\"\n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_train = x_train.reshape((len(x_train), -1))\n",
    "    x_test = x_test.reshape((len(x_test), -1))\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f78d83",
   "metadata": {},
   "source": [
    "**Bloque 2:** Autoencoder básico\n",
    "\n",
    "- **`construir_autoencoder()`** \n",
    "Crea el modelo de autoencoder básico para compresión y reconstrucción de imágenes.\n",
    "\n",
    "- **`entrenar_autoencoder()`** \n",
    "Entrena el autoencoder básico usando imágenes limpias como entrada y salida.\n",
    "\n",
    "---\n",
    "\n",
    "##### `Decisiones de diseño`\n",
    "\n",
    "##### **Justificación de la construcción del Autoencoder**\n",
    "\n",
    "- El autoencoder que se construyó se basa en una arquitectura típica con un codificador y un decodificador. El codificador reduce la dimensión de entrada (en este caso, 784 píxeles de imágenes MNIST aplanadas) a un espacio latente más pequeño, pasando por capas con 128 y luego 64 neuronas. Esto comprime la información relevante. El decodificador realiza el proceso inverso, expandiendo nuevamente la representación comprimida para reconstruir la imagen original. Esta estructura permite que la red aprenda una representación compacta pero informativa de los datos.\n",
    "\n",
    "- Para las capas ocultas se utilizó la función de activación ReLU (Rectified Linear Unit). ReLU es una función no lineal que transforma la entrada dejando pasar los valores positivos y poniendo a cero los negativos. Esto es importante porque la no linealidad permite al modelo capturar patrones complejos en los datos, mientras que su simplicidad computacional facilita el entrenamiento eficiente y reduce problemas como el desvanecimiento del gradiente en redes profundas. Por ello, es común y efectivo usar ReLU en las capas intermedias tanto del codificador como del decodificador.\n",
    "\n",
    "- En la capa de salida, sin embargo, se usó una activación sigmoide que mapea los valores a un rango entre 0 y 1. Esto es coherente con el preprocesamiento que hicimos al normalizar las imágenes MNIST a ese mismo rango. De esta forma, la red puede generar una reconstrucción que se interpreta como probabilidades o intensidades normalizadas de píxeles, lo que facilita que la salida tenga sentido visual y numérico para imágenes.\n",
    "\n",
    "- En cuanto a la función de pérdida, se eligió binary crossentropy. Aunque este criterio se asocia tradicionalmente con tareas de clasificación binaria, resulta muy efectivo para medir la diferencia entre imágenes cuyos píxeles son valores en [0,1]. La pérdida se calcula píxel a píxel, comparando la probabilidad de píxel reconstruido con el valor original. Esto penaliza fuertemente las diferencias en cada píxel y suele ayudar a que el autoencoder aprenda a reconstruir detalles finos de las imágenes, algo que a veces la pérdida cuadrática media (MSE) no logra tan eficientemente.\n",
    "\n",
    "- Finalmente, el uso del optimizador Adam permite un entrenamiento más rápido y estable. Adam adapta el aprendizaje de cada peso de forma individual combinando el método de momento y ajuste adaptativo del learning rate, facilitando la convergencia del modelo con menos ajustes manuales.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de949d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Construye y compila un autoencoder básico con arquitectura 784 → 128 → 64 → 128 → 784.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensión de entrada de los datos (por ejemplo, 784 para MNIST).\n",
    "\n",
    "    Returns:\n",
    "        Model: Modelo Keras compilado del autoencoder.\n",
    "    \"\"\"\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    # Codificador\n",
    "    encoded = Dense(128, activation='relu')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    # Decodificador\n",
    "    decoded = Dense(128, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    # Modelo completo\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "    return autoencoder\n",
    "\n",
    "def entrenar_autoencoder(autoencoder, x_train, x_test, epochs=20, batch_size=256):\n",
    "    \"\"\"\n",
    "    Entrena un autoencoder con imágenes limpias como entrada y salida.\n",
    "\n",
    "    Args:\n",
    "        autoencoder (Model): Modelo Keras del autoencoder.\n",
    "        x_train (ndarray): Datos de entrenamiento.\n",
    "        x_test (ndarray): Datos de validación.\n",
    "        epochs (int, opcional): Número de épocas de entrenamiento. Default 20.\n",
    "        batch_size (int, opcional): Tamaño de lote. Default 256.\n",
    "\n",
    "    Returns:\n",
    "        History: Objeto con el historial de entrenamiento.\n",
    "    \"\"\"\n",
    "    history_A = autoencoder.fit(\n",
    "        x_train, x_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, x_test),\n",
    "        verbose=0\n",
    "    )\n",
    "    return history_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd113548",
   "metadata": {},
   "source": [
    "**Bloque 3:** Autoencoder para denoising\n",
    "\n",
    "- **`generar_ruido()`** \n",
    "Genera versiones ruidosas del dataset manteniendo valores en [0, 1].\n",
    "\n",
    "- **`entrenar_denoising_autoencoder()`** \n",
    "Entrena un autoencoder para reconstruir imágenes limpias a partir de entradas ruidosas.\n",
    "\n",
    "---\n",
    "\n",
    "##### `Decisiones de diseño`\n",
    "\n",
    "##### **Justificación del Denoising Autoencoder:**\n",
    "\n",
    "- En esta parte del trabajo se utilizó ruido gaussiano para generar versiones ruidosas de las imágenes originales. El propósito de esto es entrenar un denoising autoencoder, que es un modelo diseñado para aprender a limpiar o eliminar ruido de los datos de entrada. Al presentar al modelo imágenes con ruido como entrada y las imágenes originales limpias como salida esperada, se fuerza al autoencoder a aprender representaciones más robustas y generalizables. Esto permite que el modelo no solo reconstruya las imágenes originales, sino que también aprenda a filtrar y corregir perturbaciones, mejorando su capacidad para manejar datos ruidosos en aplicaciones reales.\n",
    "\n",
    "- Respecto a la arquitectura, en esta etapa no se vuelve a definir ni modificar la estructura del autoencoder (es decir, no se repite explícitamente la reducción dimensional de 784 a 64 y la expansión inversa). Esto se debe a que se reutiliza el mismo modelo previamente construido en la parte anterior de autoencoder. El denoising autoencoder comparte la misma arquitectura porque la tarea fundamental sigue siendo la reconstrucción, pero ahora con una dificultad mayor: limpiar el ruido. Por tanto, no es necesario cambiar la estructura, sino solo entrenar el modelo con diferentes datos de entrada y salida.\n",
    "\n",
    "- Esta estrategia permite aprovechar una arquitectura bien probada y evita sobrecomplicar el código, enfocándose en cómo se entrena el modelo más que en cómo se diseña la red en sí.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d415e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_ruido(x_train, x_test, noise_factor=0.5):\n",
    "    \"\"\"\n",
    "    Añade ruido gaussiano a las imágenes y asegura que los valores queden en [0, 1].\n",
    "\n",
    "    Args:\n",
    "        x_train (ndarray): Datos de entrenamiento.\n",
    "        x_test (ndarray): Datos de prueba.\n",
    "        noise_factor (float, opcional): Intensidad del ruido. Default 0.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x_train_noisy, x_test_noisy) con las imágenes ruidosas.\n",
    "    \"\"\"\n",
    "    x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "    x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "    # Recortar valores al rango [0, 1]\n",
    "    x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "    x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "    return x_train_noisy, x_test_noisy\n",
    "\n",
    "def entrenar_denoising_autoencoder(autoencoder, x_train_noisy, x_train, x_test_noisy, x_test, epochs=20, batch_size=256):\n",
    "    \"\"\"\n",
    "    Entrena un autoencoder para eliminar ruido, usando imágenes ruidosas como entrada\n",
    "    y limpias como salida.\n",
    "\n",
    "    Args:\n",
    "        autoencoder (Model): Modelo base del autoencoder.\n",
    "        x_train_noisy (ndarray): Datos de entrenamiento ruidosos.\n",
    "        x_train (ndarray): Datos de entrenamiento limpios.\n",
    "        x_test_noisy (ndarray): Datos de validación ruidosos.\n",
    "        x_test (ndarray): Datos de validación limpios.\n",
    "        epochs (int, opcional): Número de épocas. Default 20.\n",
    "        batch_size (int, opcional): Tamaño de lote. Default 256.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (denoising_autoencoder, history) con el modelo entrenado y su historial.\n",
    "    \"\"\"\n",
    "    denoising_autoencoder = Model(autoencoder.input, autoencoder.output)  # misma arquitectura\n",
    "    denoising_autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "    history = denoising_autoencoder.fit(\n",
    "        x_train_noisy, x_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test_noisy, x_test),\n",
    "        verbose=0\n",
    "    )\n",
    "    return denoising_autoencoder, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6c38f",
   "metadata": {},
   "source": [
    "**Bloque 4:** Comparación de modelos\n",
    "\n",
    "- **`graficar_perdidas_comparadas()`** \n",
    "Visualizala evolución comparativa de las pérdidas de entrenamiento y validación de un autoencoder básico y un denoising autoencoder.\n",
    "\n",
    "- **`mostrar_reconstrucciones_combinadas()`** \n",
    "Muestra una figura con las imágenes originales, las reconstrucciones del autoencoder básico, las imágenes ruidosas y las reconstrucciones del denoising autoencoder para comparar visualmente su desempeño.\n",
    "\n",
    "- **`comparar_perdidas()`** \n",
    "Muestra una tabla comparativa de las pérdidas de entrenamiento y validación de ambos modelos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_perdidas_comparadas(history_A, history_B):\n",
    "    \"\"\"\n",
    "    Grafica en un solo gráfico la comparación de las pérdidas de entrenamiento y validación \n",
    "    de dos modelos: el autoencoder básico y el denoising autoencoder.\n",
    "\n",
    "    Args:\n",
    "        history_A (History): Objeto History retornado por el entrenamiento del autoencoder básico.\n",
    "        history_B (History): Objeto History retornado por el entrenamiento del denoising autoencoder.\n",
    "\n",
    "    La gráfica incluye cuatro curvas:\n",
    "        - Pérdida de entrenamiento del autoencoder básico.\n",
    "        - Pérdida de validación del autoencoder básico.\n",
    "        - Pérdida de entrenamiento del denoising autoencoder.\n",
    "        - Pérdida de validación del denoising autoencoder.\n",
    "    \"\"\"\n",
    "    plt.plot(history_A.history['loss'], label='Entrenamiento Autoencoder')\n",
    "    plt.plot(history_A.history['val_loss'], label='Validación Autoencoder')\n",
    "    plt.plot(history_B.history['loss'], label='Entrenamiento Denoising')\n",
    "    plt.plot(history_B.history['val_loss'], label='Validación Denoising')\n",
    "    plt.title(\"Comparación de pérdidas de reconstrucción\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Binary Crossentropy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mostrar_reconstrucciones_combinadas(autoencoder, denoising_autoencoder, x_test, x_test_noisy, n=10):\n",
    "    \"\"\"\n",
    "    Muestra una figura con reconstrucciones comparativas de dos modelos: el autoencoder básico y \n",
    "    el denoising autoencoder, junto con las imágenes originales y ruidosas.\n",
    "\n",
    "    La visualización consta de 4 filas y n columnas, donde cada fila representa:\n",
    "        1. Imágenes originales (limpias).\n",
    "        2. Reconstrucciones del autoencoder básico.\n",
    "        3. Imágenes con ruido añadido.\n",
    "        4. Reconstrucciones del denoising autoencoder.\n",
    "\n",
    "    Args:\n",
    "        autoencoder (Model): Modelo Keras entrenado del autoencoder básico.\n",
    "        denoising_autoencoder (Model): Modelo Keras entrenado del denoising autoencoder.\n",
    "        x_test (ndarray): Imágenes originales de prueba (sin ruido).\n",
    "        x_test_noisy (ndarray): Imágenes de prueba con ruido añadido.\n",
    "        n (int, opcional): Número de imágenes a mostrar por fila. Por defecto 10.\n",
    "\n",
    "    La figura utiliza GridSpec para reservar una columna adicional a la izquierda, donde se colocan\n",
    "    etiquetas que identifican cada fila (por ejemplo, \"Originales\", \"Reconstrucción Autoencoder\", etc.).\n",
    "    \"\"\"\n",
    "    decoded_basic = autoencoder.predict(x_test)\n",
    "    decoded_denoise = denoising_autoencoder.predict(x_test_noisy)\n",
    "\n",
    "    etiquetas_filas = [\"Originales\", \"Reconstrucción\\nAutoencoder\", \"Ruidosas\", \"Reconstrucción\\nDenoising\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(2 + n*2, 8))  # un poco de ancho extra para etiquetas\n",
    "\n",
    "    gs = GridSpec(4, n+1, width_ratios=[1.5, *([2]*n)], wspace=0.05, hspace=0.1)\n",
    "\n",
    "    for fila in range(4):\n",
    "        # Etiqueta en la primera columna de la fila, centrada verticalmente\n",
    "        ax_label = fig.add_subplot(gs[fila, 0])\n",
    "        ax_label.text(0.5, 0.5, etiquetas_filas[fila], fontsize=12, ha='center', va='center', rotation=0)\n",
    "        ax_label.axis('off')\n",
    "\n",
    "        for col in range(n):\n",
    "            ax = fig.add_subplot(gs[fila, col+1])\n",
    "            if fila == 0:\n",
    "                ax.imshow(x_test[col].reshape(28, 28), cmap='gray')\n",
    "            elif fila == 1:\n",
    "                ax.imshow(decoded_basic[col].reshape(28, 28), cmap='gray')\n",
    "            elif fila == 2:\n",
    "                ax.imshow(x_test_noisy[col].reshape(28, 28), cmap='gray')\n",
    "            else:  # fila == 3\n",
    "                ax.imshow(decoded_denoise[col].reshape(28, 28), cmap='gray')\n",
    "\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def comparar_perdidas(history_A, history_B):\n",
    "    \"\"\"\n",
    "    Crea y muestra una tabla comparativa de pérdidas y validaciones entre\n",
    "    un autoencoder básico y un denoising autoencoder.\n",
    "\n",
    "    Args:\n",
    "        history_A (History): Historial del entrenamiento del autoencoder básico.\n",
    "        history_B (History): Historial del entrenamiento del denoising autoencoder.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Crea y muestra una tabla comparativa de pérdidas y validaciones\n",
    "    entre un autoencoder básico y un denoising autoencoder.\n",
    "\n",
    "    Parámetros:\n",
    "        history_A: objeto History del entrenamiento del autoencoder básico\n",
    "        history_B: objeto History del entrenamiento del denoising autoencoder\n",
    "    \"\"\"\n",
    "    loss_A = history_A.history['loss']\n",
    "    val_loss_A = history_A.history['val_loss']\n",
    "    loss_B = history_B.history['loss']\n",
    "    val_loss_B = history_B.history['val_loss']\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Epoch': list(range(1, len(loss_A) + 1)),\n",
    "        'Loss_Autoencoder': loss_A,\n",
    "        'Val_Loss_Autoencoder': val_loss_A,\n",
    "        'Loss_Denoising': loss_B,\n",
    "        'Val_Loss_Denoising': val_loss_B\n",
    "    })\n",
    "\n",
    "    display(df.style.hide(axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750976f",
   "metadata": {},
   "source": [
    "**Bloque 4:** Función de ejecución main.\n",
    "\n",
    "- **`main()`** \n",
    "Función principal que ejecuta el flujo completo de entrenamiento, evaluación y comparación de un autoencoder básico y un denoising autoencoder sobre MNIST.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función que ejecuta el flujo completo: carga y preprocesa datos, construye y entrena \n",
    "    el autoencoder básico, genera datos ruidosos, entrena el denoising autoencoder,\n",
    "    grafica las pérdidas y visualiza reconstrucciones, y finalmente compara las pérdidas\n",
    "    de ambos modelos en una tabla.\n",
    "    \"\"\"\n",
    "    # Carga y preprocesamiento\n",
    "    print(\"Cargando datos\")\n",
    "    x_train, x_test = cargar_datos()\n",
    "\n",
    "    # Construcción y entrenamiento del autoencoder básico\n",
    "    print(\"Construyendo y entrenando Autoencoder básico\")\n",
    "    autoencoder = construir_autoencoder(input_dim=x_train.shape[1])\n",
    "    history_A = entrenar_autoencoder(autoencoder, x_train, x_test)\n",
    "\n",
    "    # Generación de ruido y entrenamiento del denoising autoencoder\n",
    "    print(\"Generando ruido y entrenando Denoising Autoencoder\")\n",
    "    x_train_noisy, x_test_noisy = generar_ruido(x_train, x_test)\n",
    "    denoising_autoencoder, history_B = entrenar_denoising_autoencoder(\n",
    "        autoencoder, x_train_noisy, x_train, x_test_noisy, x_test\n",
    "    )\n",
    "\n",
    "    # Gráficas de pérdidas comparadas entre ambos modelos\n",
    "    print(\"=\"*40)\n",
    "    print(\"Gráficas de pérdidas\")\n",
    "    print(\"=\"*40)\n",
    "    graficar_perdidas_comparadas(history_A, history_B)\n",
    "\n",
    "    # Reconstrucciones comparadas entre ambos modelos\n",
    "    print(\"=\"*40)\n",
    "    print(\"Reconstrucciones comparadas\")\n",
    "    print(\"=\"*40)\n",
    "    mostrar_reconstrucciones_combinadas(autoencoder, denoising_autoencoder, x_test, x_test_noisy, n=10)\n",
    "\n",
    "    # Comparación de pérdidas entre ambos modelos\n",
    "    print(\"=\"*40)\n",
    "    print(\"Comparación de pérdidas\")\n",
    "    print(\"=\"*40)\n",
    "    comparar_perdidas(history_A, history_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90a7c0",
   "metadata": {},
   "source": [
    "# 4. Visualización de resultados\n",
    "\n",
    "Se muestran los resultados obtenidos a partir de la ejecución de la funcion **main()**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bc682",
   "metadata": {},
   "source": [
    "# 5. Análisis de los resultados y reflexiones finales\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Evaluación de resultados entre los dos modelos\n",
    "- La comparación de las pérdidas de entrenamiento y validación muestra que el autoencoder básico presenta consistentemente valores más bajos que el denoising autoencoder. Esto indica que, en términos numéricos, el modelo básico es más eficiente reconstruyendo imágenes limpias sin ruido, logrando una mayor precisión en la reconstrucción.\n",
    "\n",
    "- Por otro lado, el denoising autoencoder presenta pérdidas más altas, lo cual es esperable debido a la mayor complejidad de su tarea: debe reconstruir imágenes limpias a partir de versiones ruidosas. Esta dificultad adicional influye en que el modelo no alcance una pérdida tan baja como el autoencoder básico.\n",
    "\n",
    "- En cuanto a la evaluación visual, el autoencoder básico tiende a producir reconstrucciones casi idénticas a las imágenes originales, si bien no son exactamente iguales, logran mostrar una alta fidelidad en la reproducción de los detalles. Mientras tanto, el denoising autoencoder cumple con su función de eliminar ruido de las imágenes, pero a menudo puede suavizar detalles finos o introducir pequeñas distorsiones. La calidad visual del denoising depende en gran medida de la cantidad de ruido presente y de la capacidad del modelo para generalizar y eliminar eficazmente las imperfecciones sin degradar la información relevante, en este caso, las reconstrucciones aunque buenas, no alcanzaron el mismo nivel que las reconstruidas cono el autoencoder básico.\n",
    "\n",
    "- En resumen:\n",
    "    - El autoencoder básico presenta consistentemente pérdidas de entrenamiento y validación más bajas que el denoising autoencoder.\n",
    "    - Esto indica que, en términos numéricos, el modelo básico reconstruye mejor las imágenes limpias sin ruido.\n",
    "    - El denoising autoencoder tiene pérdidas más altas, lo cual es esperable dado que su tarea es más compleja: reconstruir imágenes limpias a partir de versiones ruidosas.\n",
    "\n",
    "## 2. Reflexión sobre el potencial uso de autoencoders y denoising autoencoders\n",
    "- Las técnicas de autoencoders, y particularmente los denoising autoencoders, tienen un enorme potencial en campos como medicina, seguridad e industria: \n",
    "\n",
    "    - En medicina, estos modelos pueden utilizarse para mejorar la calidad de imágenes médicas (como resonancias magnéticas, tomografías o rayos X) eliminando ruido o artefactos que dificultan el diagnóstico. Esto puede ayudar a los profesionales a detectar anomalías con mayor precisión y a reducir falsos positivos o negativos, lo que impacta directamente en la calidad del tratamiento y pronóstico.\n",
    "\n",
    "    - En seguridad, los autoencoders pueden emplearse para la detección de anomalías en señales o datos, como identificar accesos no autorizados, fraudes o comportamientos irregulares en sistemas complejos. La capacidad de reconstruir datos esperados y detectar desviaciones hace que sean una herramienta valiosa para proteger infraestructuras críticas y datos sensibles.\n",
    "\n",
    "    - En la industria, los autoencoders pueden mejorar el mantenimiento predictivo, analizando señales de sensores en maquinaria para detectar fallos incipientes. La eliminación de ruido en estas señales permite una monitorización más fiable y la anticipación de problemas, evitando costosas interrupciones y optimizando recursos.\n",
    "\n",
    "- En resumen, el uso de autoencoders y denoising autoencoders aporta un equilibrio entre la reducción de dimensionalidad, limpieza de datos y detección de patrones, convirtiéndolos en herramientas fundamentales para mejorar procesos, aumentar la seguridad y apoyar la toma de decisiones en entornos cada vez más complejos y automatizados.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
