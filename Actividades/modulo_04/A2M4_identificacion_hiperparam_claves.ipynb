{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993b323c",
   "metadata": {},
   "source": [
    "# Actividad 2:\n",
    "# Comparativa de técnicas de ajuste de hiperparámetros en clasificación médica\n",
    "\n",
    "## Objetivo\n",
    "Construir un modelo de clasificación para predecir la probabilidad de diabetes en pacientes usando el conjunto de datos Pima Indians Diabetes Dataset, aplicando Grid Search y Random Search para optimizar el rendimiento del modelo y comparar sus resultados en términos de precisión, F1-score y eficiencia computacional.\n",
    "\n",
    "**Dataset utilizado:**  \n",
    "[Pima Indians Diabetes Dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "\n",
    "---\n",
    "\n",
    "### Estructura del Notebook:\n",
    "1. Metodología.\n",
    "2. Configuración inicial del notebook.\n",
    "3. Definicion de funciones.\n",
    "4. Uso de funciones y resultados.\n",
    "5. Análisis de los resultados y reflexiones finales.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6df00",
   "metadata": {},
   "source": [
    "## 1. Metodología\n",
    "\n",
    "---\n",
    "\n",
    "### Flujo de trabajo\n",
    "1. **Carga y exploración de datos:**\n",
    "   - Estadísticas descriptivas básicas.\n",
    "\n",
    "2. **Preprocesamiento:**\n",
    "   - Detección y tratamiento de valores anómalos en base a un rango de valores biológicamente viables. Los datos fuera de este rango (outliers) distintos de cero fueron medidos en % de la muestra total, si era menor al 1% los datos se borraban, mientras que si eran mas del 1% se transformaban en cero. Para valores cero en variables en donde este valor es INCOMPATIBLE con la vida, se conviertieron en % igual en que el caso de los fuera de rango, en donde si el numero de muestras con cero era menor al 1% del total, se borraban, de otra maneta, se decidió transformar en mediana dichos valores que fueran mayores al 1%.\n",
    "   - Escalado de variables numéricas (StandardScaler).\n",
    "   - División estratificada del dataset (70% entrenamiento - 30% prueba).\n",
    "\n",
    "3. **Modelado y optimización:**\n",
    "   - Modelo base: Random Forest sin ajuste\n",
    "   - Técnicas de optimización:\n",
    "     - Grid Search (búsqueda exhaustiva en malla paramétrica)\n",
    "     - Random Search (muestreo aleatorio de parámetros)\n",
    "   - Métricas de evaluación: \n",
    "      - F1-Score: es la media armónica de la precisión y el recall, especialmente útil cuando hay un desequilibrio de clases en los datos.\n",
    "      - Precisión: mide la exactitud de las predicciones positivas del modelo. \"¿Cuantos predicciones positivas son realmente positivas?\"\n",
    "      - Recall: mide la capacidad del modelo para identificar todas las instancias positivas reales. \"¿De todas las instancias positivas, cuantas predijo correctamente?\"\n",
    "      - AUC: métrica que mide la capacidad de un modelo de clasificación para distinguir entre clases. La curva ROC traza la tasa de verdaderos positivos (Recall) contra la tasa de falsos positivos (FPR) en varios umbrales de clasificación.\n",
    "\n",
    "4. **Evaluación comparativa:**\n",
    "   - Análisis de rendimiento (métricas).\n",
    "   - Comparación de tiempos de ejecución.\n",
    "   - Visualización de resultados.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda30d1",
   "metadata": {},
   "source": [
    "# 2. Configuración inicial del notebook\n",
    "- Importación de librerias necesarias.\n",
    "- Configuraciones necesarias para el correcto manejo de las salidas del código.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d62666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configuración de pandas para mostrar todos los parámetros\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69ae47",
   "metadata": {},
   "source": [
    "# 3. Definición de funciones\n",
    "\n",
    "> **Nota:** Para mejor comprensión de las funciones y su utilidad, esta sección se divide en bloques, en donde cada uno responde a una parte diferente de la metodología de trabajo. \n",
    "\n",
    "> **Nota 2:** Los nombres de las columnas se definieron en base a la informacion disponible en `kaggle` sobre este data set: \n",
    "[Pima Indians Diabetes Dataset (Kaggle)](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database), ya que en el link de github no se encontraban.\n",
    "\n",
    "---\n",
    "\n",
    "**Bloque 1:** Funciones de preprocesamiento de datos.\n",
    "\n",
    "- **`load_data()`** \n",
    "Carga el dataset, define las columnas y lo convierte en un DataFrame con **pandas**. Luego hace una exploración inicial del df mostrando estadísticas básicas, además de la distribución de personas con y sin diabetes.\n",
    "\n",
    "- **`preprocess_data()`** \n",
    "Preprocesa el dataframe tratando outliers, escalando los datos y finalmente dividiendo el dataset en datos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f910e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(url: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Carga un dataset de diabetes desde una URL, asigna nombres de columnas y entrega estadísticas exploratorias.\n",
    "\n",
    "    Este proceso incluye:\n",
    "    - Lectura del archivo CSV desde la URL proporcionada.\n",
    "    - Asignación de nombres a las columnas según el dataset de diabetes de PIMA.\n",
    "    - Cálculo de estadísticas básicas (media, mediana, moda, percentiles, mínimo y máximo).\n",
    "    - Cálculo de la distribución porcentual de la variable objetivo `Outcome`.\n",
    "\n",
    "    Args:\n",
    "        url (str): Ruta o URL del archivo CSV que contiene el dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - df (pd.DataFrame): DataFrame original con los datos cargados.\n",
    "            - stats_df (pd.DataFrame): Estadísticas descriptivas por variable numérica.\n",
    "            - outcome_dist (pd.Series): Distribución porcentual del Outcome (0 = no diabético, 1 = diabético).\n",
    "    \"\"\"\n",
    "    # Asignación de nombres de columna\n",
    "    column_names = [\n",
    "        \"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\",\n",
    "        \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"\n",
    "    ]\n",
    "    df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "    # Estadísticas básicas\n",
    "    stats_df = pd.DataFrame({\n",
    "        \"mean\": df.mean(),\n",
    "        \"median\": df.median(),\n",
    "        \"mode\": df.mode().iloc[0],\n",
    "        \"25%\": df.quantile(0.25),\n",
    "        \"75%\": df.quantile(0.75),\n",
    "        \"min\": df.min(),\n",
    "        \"max\": df.max()\n",
    "    })\n",
    "\n",
    "    # Distribución porcentual de Outcome\n",
    "    outcome_dist = df[\"Outcome\"].value_counts(normalize=True) * 100\n",
    "    outcome_dist = outcome_dist.rename(\"percentage (%)\")\n",
    "\n",
    "    return df, stats_df, outcome_dist\n",
    "\n",
    "def prepare_data(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"Outcome\",\n",
    "    test_size: float = 0.3,\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Prepara el dataset para entrenamiento aplicando limpieza, imputación, escalado y división.\n",
    "\n",
    "    El proceso completo incluye:\n",
    "    1. Detección y tratamiento de valores fuera de rango (outliers) distintos de cero:\n",
    "        - Reemplazo con cero si superan el 1% de la columna.\n",
    "        - Eliminación de filas si representan menos del 1%.\n",
    "    2. Imputación de ceros usando la mediana (si hay más del 1%) o eliminación de filas.\n",
    "    3. Escalado de variables numéricas con StandardScaler.\n",
    "    4. División estratificada del dataset en conjunto de entrenamiento y prueba (train/test).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original con los datos.\n",
    "        target_col (str, opcional): Nombre de la columna objetivo. Por defecto es \"Outcome\".\n",
    "        test_size (float, opcional): Proporción del dataset para el conjunto de prueba. Default = 0.3.\n",
    "        random_state (int, opcional): Semilla aleatoria para reproducibilidad. Default = 42.\n",
    "        verbose (bool, opcional): Si True, imprime detalles del proceso. Default = True.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - X_train (np.ndarray): Conjunto de entrenamiento (features escaladas).\n",
    "            - X_test (np.ndarray): Conjunto de prueba (features escaladas).\n",
    "            - y_train (pd.Series): Etiquetas de entrenamiento.\n",
    "            - y_test (pd.Series): Etiquetas de prueba.\n",
    "            - scaler (StandardScaler): Objeto escalador ajustado.\n",
    "    \"\"\"\n",
    "    # 1. Limpieza\n",
    "    valid_ranges = {\n",
    "        \"Glucose\": (40, 500),\n",
    "        \"BloodPressure\": (40, 180),\n",
    "        \"SkinThickness\": (10, 80),\n",
    "        \"Insulin\": (10, 900),\n",
    "        \"BMI\": (10, 70),\n",
    "        \"DiabetesPedigreeFunction\": (0.05, 2.5),\n",
    "        \"Age\": (10, 120)\n",
    "    }\n",
    "\n",
    "    df_clean = df.copy()\n",
    "    original_rows = df.shape[0]\n",
    "    dropped_1, zeroed_1 = 0, 0\n",
    "\n",
    "    # 1. Tratamiento de outliers distintos de cero\n",
    "    if verbose:\n",
    "        print(\"=== Limpieza de valores fuera de rango (excluyendo ceros) ===\")\n",
    "    \n",
    "    for col, (min_val, max_val) in valid_ranges.items():\n",
    "        # Detectar valores fuera de rango que NO sean cero\n",
    "        non_zero_mask = df_clean[col] != 0\n",
    "        outliers = non_zero_mask & ((df_clean[col] < min_val) | (df_clean[col] > max_val))\n",
    "        \n",
    "        pct_out = outliers.mean() * 100\n",
    "        count_out = outliers.sum()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{col}: {pct_out:.2f}% ({count_out}) valores no-cero fuera de rango\")\n",
    "        \n",
    "        if pct_out > 1:\n",
    "            # Reemplazar SOLO valores fuera de rango (no-cero) con 0\n",
    "            df_clean.loc[outliers, col] = 0\n",
    "            zeroed_1 += count_out\n",
    "        else:\n",
    "            # Eliminar filas con valores fuera de rango (no-cero)\n",
    "            df_clean = df_clean[~outliers]\n",
    "            dropped_1 += count_out\n",
    "\n",
    "    # 2. Imputación de ceros\n",
    "    dropped_2, imputed_2 = 0, 0\n",
    "    if verbose:\n",
    "        print(\"\\n=== Tratamiento de ceros ===\")\n",
    "    \n",
    "    # Convertir columnas numéricas a float antes de la imputación\n",
    "    float_cols = [\"BMI\", \"DiabetesPedigreeFunction\", \"Insulin\"]\n",
    "    for col in float_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype(float)\n",
    "    \n",
    "    for col in valid_ranges:\n",
    "        zeros = df_clean[col] == 0\n",
    "        pct_zeros = zeros.mean() * 100\n",
    "        count_zeros = zeros.sum()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{col}: {pct_zeros:.2f}% ceros ({count_zeros})\")\n",
    "        \n",
    "        if pct_zeros > 1:\n",
    "            # Calcular mediana y convertir al tipo de dato original\n",
    "            median = df_clean.loc[df_clean[col] != 0, col].median()\n",
    "            \n",
    "            # Preservar tipo de dato\n",
    "            if df_clean[col].dtype == int:\n",
    "                median = round(median)\n",
    "            \n",
    "            df_clean.loc[zeros, col] = median\n",
    "            imputed_2 += count_zeros\n",
    "        else:\n",
    "            df_clean = df_clean.loc[~zeros]\n",
    "            dropped_2 += count_zeros\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n=== Resumen limpieza ===\")\n",
    "        print(f\"Filas originales: {original_rows}\")\n",
    "        print(f\"Filas tras limpieza: {df_clean.shape[0]}\")\n",
    "        print(f\"Eliminados por outliers: {dropped_1}\")\n",
    "        print(f\"Seteados a cero por outliers: {zeroed_1}\")\n",
    "        print(f\"Eliminados por ceros: {dropped_2}\")\n",
    "        print(f\"Imputados con mediana: {imputed_2}\")\n",
    "\n",
    "    # 3. Preprocesamiento\n",
    "    X = df_clean.drop(columns=[target_col])\n",
    "    y = df_clean[target_col]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd538f",
   "metadata": {},
   "source": [
    "**Bloque 2:** Funciones de modelado.\n",
    "\n",
    "- **`train_base_model()`** \n",
    "Entrena un modelo base con parametros por defecto para futuras comparaciones de metricas, incluyendo el tiempo.\n",
    "\n",
    "- **`evaluate_model()`** \n",
    "Evalúa el rendimiento del modelo dado sobre el conjunto de datos de prueba, incluyendo las métricas a evaluar como F1-Score, precisión, recall y AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Entrena un modelo base Random Forest con parámetros por defecto.\n",
    "\n",
    "    El modelo se entrena con:\n",
    "    - Pesos balanceados para clases desbalanceadas (`class_weight='balanced'`).\n",
    "    - Uso de todos los núcleos disponibles (`n_jobs=-1`).\n",
    "    - Semilla fija para reproducibilidad (`random_state=42`).\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray o pd.DataFrame): Características del conjunto de entrenamiento.\n",
    "        y_train (pd.Series o np.ndarray): Etiquetas del conjunto de entrenamiento.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - model (RandomForestClassifier): Modelo entrenado.\n",
    "            - duration (float): Tiempo de entrenamiento en segundos.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    model = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, time.time() - start\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo entrenado en el conjunto de prueba utilizando varias métricas de clasificación.\n",
    "\n",
    "    Las métricas calculadas son:\n",
    "    - F1-Score\n",
    "    - Precisión\n",
    "    - Recall\n",
    "    - AUC (Area Under the ROC Curve)\n",
    "\n",
    "    Args:\n",
    "        model (RandomForestClassifier u otro clasificador con predict_proba): Modelo entrenado.\n",
    "        X_test (np.ndarray o pd.DataFrame): Características del conjunto de prueba.\n",
    "        y_test (pd.Series o np.ndarray): Etiquetas verdaderas del conjunto de prueba.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con las métricas:\n",
    "            - \"F1-Score\": Valor del F1-score.\n",
    "            - \"Precisión\": Valor de la precisión.\n",
    "            - \"Recall\": Valor del recall.\n",
    "            - \"AUC\": Área bajo la curva ROC.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    return {\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "        \"Precisión\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e5aaf",
   "metadata": {},
   "source": [
    "**Bloque 3:** Funciones de optimización.\n",
    "\n",
    "- **`optimize_with_gridsearch()`** \n",
    "Optimización utilizando **GridSearchCV**.\n",
    "\n",
    "- **`optimize_with_randomsearch()`** \n",
    "Optimización utilizando **RandomizedSearchCV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b81ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_gridsearch(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Optimiza un modelo Random Forest utilizando Grid Search con validación cruzada.\n",
    "\n",
    "    El proceso evalúa combinaciones exhaustivas de hiperparámetros definidos \n",
    "    en un grid, utilizando AUC como métrica de evaluación principal.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray o pd.DataFrame): Datos de entrenamiento (features).\n",
    "        y_train (pd.Series o np.ndarray): Etiquetas del conjunto de entrenamiento.\n",
    "        X_test (np.ndarray o pd.DataFrame): Datos de prueba (features).\n",
    "        y_test (pd.Series o np.ndarray): Etiquetas del conjunto de prueba.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - best_model (RandomForestClassifier): Modelo entrenado con los mejores hiperparámetros.\n",
    "            - metrics (dict): Métricas del modelo sobre el conjunto de prueba, incluyendo:\n",
    "                - \"accuracy\": Precisión del modelo.\n",
    "                - \"recall\": Recall.\n",
    "                - \"f1\": F1-score.\n",
    "                - \"auc\": Área bajo la curva ROC.\n",
    "                - \"best_params\": Diccionario de hiperparámetros óptimos.\n",
    "                - \"train_time\": Tiempo total de entrenamiento en segundos.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 150],\n",
    "        \"max_depth\": [5, 10, 15],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"auc\": roc_auc_score(y_test, y_proba),\n",
    "        \"best_params\": grid_search.best_params_,\n",
    "        \"train_time\": end - start\n",
    "    }\n",
    "\n",
    "    return best_model, metrics\n",
    "\n",
    "def optimize_with_randomsearch(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Optimiza un modelo Random Forest utilizando Randomized Search con validación cruzada.\n",
    "\n",
    "    A diferencia del Grid Search, este método explora aleatoriamente combinaciones\n",
    "    de hiperparámetros en un espacio definido, lo cual puede reducir el tiempo de cómputo.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray o pd.DataFrame): Datos de entrenamiento (features).\n",
    "        y_train (pd.Series o np.ndarray): Etiquetas del conjunto de entrenamiento.\n",
    "        X_test (np.ndarray o pd.DataFrame): Datos de prueba (features).\n",
    "        y_test (pd.Series o np.ndarray): Etiquetas del conjunto de prueba.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - best_model (RandomForestClassifier): Modelo entrenado con los mejores hiperparámetros.\n",
    "            - metrics (dict): Métricas del modelo sobre el conjunto de prueba, incluyendo:\n",
    "                - \"accuracy\": Precisión del modelo.\n",
    "                - \"recall\": Recall.\n",
    "                - \"f1\": F1-score.\n",
    "                - \"auc\": Área bajo la curva ROC.\n",
    "                - \"best_params\": Diccionario de hiperparámetros óptimos.\n",
    "                - \"train_time\": Tiempo total de entrenamiento en segundos.\n",
    "    \"\"\"\n",
    "    param_dist = {\n",
    "        \"n_estimators\": randint(50, 200),\n",
    "        \"max_depth\": randint(5, 30),\n",
    "        \"min_samples_split\": randint(2, 20)\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    random_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=100,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"auc\": roc_auc_score(y_test, y_proba),\n",
    "        \"best_params\": random_search.best_params_,\n",
    "        \"train_time\": end - start\n",
    "    }\n",
    "\n",
    "    return best_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637d2b4",
   "metadata": {},
   "source": [
    "**Bloque 4:** Funciones de visualización.\n",
    "\n",
    "- **`create_comparison_table()`** \n",
    "Crea un DataFrame con la comparación de las métricas del modelo base y las dos optimizaciones hechas.\n",
    "\n",
    "- **`visualize_results()`** \n",
    "Crea dos gráficos que comparan el rendimiento y los tiempos respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(results_dict):\n",
    "    \"\"\"\n",
    "    Crea una tabla comparativa con métricas de desempeño para distintos modelos.\n",
    "\n",
    "    La tabla incluye precisión, recall, F1-score, AUC, tiempo de entrenamiento y \n",
    "    los mejores hiperparámetros encontrados (si aplica).\n",
    "\n",
    "    Args:\n",
    "        results_dict (dict): Diccionario con resultados de modelos. Cada entrada debe tener\n",
    "            el nombre del modelo como clave y un subdiccionario con claves \"metrics\"\n",
    "            (que incluya F1-Score, accuracy, recall, AUC, etc.) y \"train_time\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame ordenado por F1-Score descendente, con columnas:\n",
    "            - Model\n",
    "            - Accuracy\n",
    "            - Recall\n",
    "            - F1-Score\n",
    "            - AUC\n",
    "            - Train Time (s)\n",
    "            - Best Params\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model_name, info in results_dict.items():\n",
    "        metrics = info.get(\"metrics\", {})\n",
    "        train_time = metrics.get(\"train_time\", None)\n",
    "        best_params = metrics.get(\"best_params\", None)\n",
    "\n",
    "        row = {\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": metrics.get(\"accuracy\", metrics.get(\"Precisión\", None)),\n",
    "            \"Recall\": metrics.get(\"recall\", metrics.get(\"Recall\", None)),\n",
    "            \"F1-Score\": metrics.get(\"f1\", metrics.get(\"F1-Score\", None)),\n",
    "            \"AUC\": metrics.get(\"auc\", metrics.get(\"AUC\", None)),\n",
    "            \"Train Time (s)\": train_time,\n",
    "            \"Best Params\": str(best_params) if best_params else \"-\"\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df_comparison = pd.DataFrame(rows)\n",
    "    return df_comparison.sort_values(by=\"F1-Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def visualize_results(df_comparison):\n",
    "    \"\"\"\n",
    "    Genera visualizaciones comparativas de desempeño entre modelos.\n",
    "\n",
    "    Crea dos gráficos:\n",
    "    - Gráfico de barras agrupadas para comparar métricas de clasificación (Accuracy, Recall, F1-Score, AUC).\n",
    "    - Gráfico de barras para comparar los tiempos de entrenamiento.\n",
    "\n",
    "    Args:\n",
    "        df_comparison (pd.DataFrame): DataFrame generado por `create_comparison_table()`, que\n",
    "            contiene métricas y tiempos de modelos entrenados.\n",
    "    \"\"\"\n",
    "    # Configuración de estilo\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(16, 7))\n",
    "    \n",
    "    # --- GRÁFICO 1: COMPARACIÓN DE MÉTRICAS ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Preparamos los datos en formato largo\n",
    "    metrics = [\"Accuracy\", \"Recall\", \"F1-Score\", \"AUC\"]\n",
    "    df_melted = df_comparison.melt(id_vars=\"Model\", \n",
    "                                  value_vars=metrics,\n",
    "                                  var_name=\"Metric\", \n",
    "                                  value_name=\"Value\")\n",
    "    \n",
    "    # Creamos el gráfico de barras agrupadas\n",
    "    palette = sns.color_palette(\"Set2\", len(df_comparison))\n",
    "    ax = sns.barplot(x=\"Metric\", \n",
    "                     y=\"Value\", \n",
    "                     hue=\"Model\", \n",
    "                     data=df_melted, \n",
    "                     palette=palette,\n",
    "                     width=0.8)  \n",
    "    \n",
    "    # Añadimos etiquetas de valores\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.3f}\", \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', \n",
    "                    xytext=(0, 9), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=9)\n",
    "    \n",
    "    plt.title(\"Comparación de Métricas por Modelo\", fontsize=16)\n",
    "    plt.xlabel(\"Métricas\", fontsize=12)\n",
    "    plt.ylabel(\"Valor\", fontsize=12)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Mover leyenda al centro superior\n",
    "    plt.legend(title=\"Modelo\", loc='upper center', ncol=3, frameon=True)\n",
    "    \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # --- GRÁFICO 2: TIEMPOS DE ENTRENAMIENTO ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Ordenamos por tiempo para mejor visualización\n",
    "    df_time = df_comparison.sort_values(\"Train Time (s)\", ascending=False)\n",
    "    \n",
    "    # Creamos el gráfico de barras para tiempos con la misma paleta\n",
    "    ax2 = sns.barplot(x=\"Model\", \n",
    "                      y=\"Train Time (s)\", \n",
    "                      data=df_time, \n",
    "                      hue=\"Model\",  # Corregimos la advertencia\n",
    "                      palette=palette,\n",
    "                      width=0.6,\n",
    "                      legend=False)\n",
    "    \n",
    "    # Añadimos etiquetas de tiempo\n",
    "    for p in ax2.patches:\n",
    "        ax2.annotate(f\"{p.get_height():.2f}s\", \n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                     ha='center', va='center', \n",
    "                     xytext=(0, 9), \n",
    "                     textcoords='offset points',\n",
    "                     fontsize=9) # Ocultamos leyenda redundante\n",
    "    \n",
    "    plt.title(\"Tiempos de Entrenamiento\", fontsize=16)\n",
    "    plt.xlabel(\"Modelos\", fontsize=12)\n",
    "    plt.ylabel(\"Segundos\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Ajustar espacio entre subplots\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    \n",
    "    plt.savefig(\"comparacion_de_modelos.png\")\n",
    "\n",
    "    # Mostrar gráficos\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b0a2b",
   "metadata": {},
   "source": [
    "**Bloque 5:** Función de ejecución.\n",
    "\n",
    "- **`main()`**\n",
    "Utiliza todas las funciones definidas anteriormente para obtener datos del modelo base y de los modelos optimizados con **GridSearchCV**, **RandomizedSearchCV** y **Optuna**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a10668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que orquesta todo el flujo de trabajo de un proyecto de clasificación\n",
    "    de diabetes tipo 2 utilizando Random Forest y diferentes métodos de optimización de hiperparámetros.\n",
    "\n",
    "    El pipeline incluye:\n",
    "        1. Carga y exploración de datos desde una URL.\n",
    "        2. Limpieza de datos y preprocesamiento (tratamiento de outliers y ceros).\n",
    "        3. Entrenamiento de un modelo base con RandomForestClassifier.\n",
    "        4. Optimización de hiperparámetros usando:\n",
    "            - Grid Search\n",
    "            - Random Search\n",
    "        5. Evaluación de los modelos usando múltiples métricas (Accuracy, Recall, F1-Score, AUC).\n",
    "        6. Comparación tabular y análisis de los resultados.\n",
    "        7. Visualización de métricas y tiempos de entrenamiento.\n",
    "\n",
    "    Esta función imprime resultados intermedios y finales en consola, incluyendo estadísticas\n",
    "    descriptivas, métricas, parámetros óptimos y análisis comparativo.\n",
    "    \"\"\"\n",
    "    # Configuración inicial\n",
    "    URL = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # 1. Carga de datos\n",
    "    print(\"\\nCargando y explorando datos...\")\n",
    "    df, stats_df, outcome_dist = load_and_explore_data(URL)\n",
    "    \n",
    "    print(\"\\nDistribución de Outcome (%):\\n\")\n",
    "    print(outcome_dist.to_string())\n",
    "    print(\"\\nEstadísticas descriptivas:\\n\")\n",
    "    print(stats_df.to_string())\n",
    "    \n",
    "    # 2. Preparación de datos\n",
    "    print(\"\\nPreparando datos...\\n\")\n",
    "    X_train, X_test, y_train, y_test, scaler = prepare_data(df, verbose=True)\n",
    "    \n",
    "    # 3. Modelo base\n",
    "    print(\"\\nEntrenando modelo base...\")\n",
    "    base_model, base_time = train_base_model(X_train, y_train)\n",
    "    base_metrics = evaluate_model(base_model, X_test, y_test)\n",
    "    print(\"\\nMétricas del modelo base:\\n\")\n",
    "    for metric, value in base_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # 4. Optimización con GridSearch\n",
    "    print(\"\\nOptimizando con GridSearch...\")\n",
    "    grid_model, grid_metrics = optimize_with_gridsearch(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 5. Optimización con RandomSearch\n",
    "    print(\"\\nOptimizando con RandomSearch...\")\n",
    "    random_model, random_metrics = optimize_with_randomsearch(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 6. Comparación de resultados\n",
    "    results_dict = {\n",
    "        \"Base Model\": {\n",
    "            \"model\": base_model,\n",
    "            \"metrics\": {\n",
    "                **base_metrics,\n",
    "                \"accuracy\": accuracy_score(y_test, base_model.predict(X_test)),\n",
    "                \"train_time\": base_time\n",
    "            }\n",
    "        },\n",
    "        \"GridSearch\": {\n",
    "            \"model\": grid_model,\n",
    "            \"metrics\": grid_metrics\n",
    "        },\n",
    "        \"RandomSearch\": {\n",
    "            \"model\": random_model,\n",
    "            \"metrics\": random_metrics\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    df_comparison = create_comparison_table(results_dict)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTADOS FINALES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Formato mejorado para la tabla\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"COMPARACIÓN DE MODELOS\")\n",
    "    print(\"-\"*70)\n",
    "    print(df_comparison.drop(columns=\"Best Params\").to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PARÁMETROS ÓPTIMOS\")\n",
    "    print(\"-\"*70)\n",
    "    for i, row in df_comparison.iterrows():\n",
    "        print(f\"\\n{row['Model']}:\")\n",
    "        if row['Best Params'] != '-':\n",
    "            params = eval(row['Best Params'])\n",
    "            for key, value in params.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"  Parámetros por defecto\")\n",
    "    \n",
    "    # Análisis de resultados\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"ANÁLISIS DE RESULTADOS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    base_accuracy = df_comparison[df_comparison['Model'] == 'Base Model']['Accuracy'].values[0]\n",
    "    grid_accuracy = df_comparison[df_comparison['Model'] == 'GridSearch']['Accuracy'].values[0]\n",
    "    random_accuracy = df_comparison[df_comparison['Model'] == 'RandomSearch']['Accuracy'].values[0]\n",
    "\n",
    "    base_recall = df_comparison[df_comparison['Model'] == 'Base Model']['Recall'].values[0]\n",
    "    grid_recall = df_comparison[df_comparison['Model'] == 'GridSearch']['Recall'].values[0]\n",
    "    random_recall = df_comparison[df_comparison['Model'] == 'RandomSearch']['Recall'].values[0]\n",
    "    \n",
    "    base_f1 = df_comparison[df_comparison['Model'] == 'Base Model']['F1-Score'].values[0]\n",
    "    grid_f1 = df_comparison[df_comparison['Model'] == 'GridSearch']['F1-Score'].values[0]\n",
    "    random_f1 = df_comparison[df_comparison['Model'] == 'RandomSearch']['F1-Score'].values[0]\n",
    "\n",
    "    print(f\"\\n1. Mejora en Accuracy:\")\n",
    "    print(f\"   - GridSearch: +{(grid_accuracy - base_accuracy)*100:.1f}% vs Base Model\")\n",
    "    print(f\"   - RandomSearch: +{(random_accuracy - base_accuracy)*100:.1f}% vs Base Model\")\n",
    "    \n",
    "    print(f\"\\n1. Mejora en Recall (detección de positivos):\")\n",
    "    print(f\"   - GridSearch: +{(grid_recall - base_recall)*100:.1f}% vs Base Model\")\n",
    "    print(f\"   - RandomSearch: +{(random_recall - base_recall)*100:.1f}% vs Base Model\")\n",
    "    \n",
    "    print(f\"\\n2. Mejora en F1-Score (equilibrio precisión-recall):\")\n",
    "    print(f\"   - GridSearch: +{(grid_f1 - base_f1)*100:.1f}% vs Base Model\")\n",
    "    print(f\"   - RandomSearch: +{(random_f1 - base_f1)*100:.1f}% vs Base Model\")\n",
    "    \n",
    "    print(f\"\\n3. Tiempo de entrenamiento:\")\n",
    "    print(f\"   - GridSearch fue {df_comparison['Train Time (s)'].iloc[0]/df_comparison['Train Time (s)'].iloc[2]:.0f}x más lento que Base Model\")\n",
    "    print(f\"   - RandomSearch fue {df_comparison['Train Time (s)'].iloc[1]/df_comparison['Train Time (s)'].iloc[2]:.0f}x más lento que Base Model\")\n",
    "\n",
    "    # Visualización\n",
    "    print(\"\\nGenerando visualizaciones...\")\n",
    "    visualize_results(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690d3e2",
   "metadata": {},
   "source": [
    "# 4. Visualización de resultados\n",
    "\n",
    "Se muestran los resultados obtenidos a partir de la ejecución de la funcion **main()**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa76593",
   "metadata": {},
   "source": [
    "# 5. Reflexión Final\n",
    "\n",
    "> **Nota:** Debido a que hay variaciones entre ejecuciones, se puede esperar que algunos datos como la comparación de eficiencia en tiempo tenga datos levemente diferentes de los que se muestren al momento de ejecutar nuevamente el notebook, por lo que se deben tomar mas como **aproximaciones** y no como un resultado exacto.\n",
    "\n",
    "## 1. ¿Cuál técnica fue más eficiente?\n",
    "- Random Search demostró ser más eficiente:\n",
    "    - Velocidad: 7s vs 16s de Grid Search.\n",
    "    - Recursos computacionales: Menor consumo de CPU/memoria.\n",
    "    - Costo-beneficio: Mejor equilibrio entre tiempo y mejora de métricas.\n",
    "    - Escalabilidad: Más adecuado para espacios de búsqueda grandes.\n",
    "\n",
    "¿Por qué sucede esto?\n",
    "- Random Search explora eficientemente espacios hiperparamétricos amplios con menos iteraciones, mientras que Grid Search sufre del \"curse of dimensionality\" - el número de combinaciones crece exponencialmente con cada nuevo parámetro añadido.\n",
    "\n",
    "## 2. ¿Cuál encontró el mejor modelo?\n",
    "- Basados en F1-Score, Random Search fue mejor modelo, principalmente en términos de:\n",
    "    - Acurracy: 2.6% de mejora en relación al modelo base, y 0.4% mejor que Grid Search.\n",
    "    - F1-Score: 2.6% mejor que el modelo base y 1.5% mejor que el Grid Search.\n",
    "    - Balance general: Mejor equilibrio entre precisión y exhaustividad\n",
    "- Recall fue igual al del modelo base, y mayor al de Grid Search, AUC fue levemente mejor en Grid Search.\n",
    "\n",
    "## 3. ¿Qué hubieras hecho diferente?\n",
    "Mejoras clave en el flujo de trabajo:\n",
    "- Probar diferentes numeros de iteraciones, ya que Random Search hace busquedas aleatorias podríamos probar espacios de busqueda mas amplios.\n",
    "- Feature Engineering avanzado:\n",
    "- Manejo de desbalanceo con SMOTE:\n",
    "- Optimización bayesiana:\n",
    "- Validación cruzada estratificada:\n",
    "- Ensamblaje de modelos:\n",
    "\n",
    "Priorización de métricas:\n",
    "\n",
    "- Para problemas médicos como detección de diabetes, hubiera enfocado más en:\n",
    "    - Maximizar Recall (minimizar falsos negativos)\n",
    "    - Optimizar curva Precision-Recall en lugar de solo AUC\n",
    "    - Usar métricas específicas como Sensibilidad y Especificidad\n",
    "\n",
    "- Análisis de características:\n",
    "    - Realizar análisis de importancia con SHAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
